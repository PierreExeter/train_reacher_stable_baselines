/home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

========== Pendulum-v0 ==========
Seed: 8
OrderedDict([('ent_coef', 0.0),
             ('gamma', 0.95),
             ('n_envs', 8),
             ('n_timesteps', 100000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy')])
Using 8 environments
Overwriting n_timesteps with n=100000
Normalizing input and reward
Creating test environment
Normalization activated: {'norm_reward': False}
WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:158: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:182: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/bin/anaconda3/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py:194: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

Log path: logs/a2c/Pendulum-v0_Env_8_norm//a2c/Pendulum-v0_9
---------------------------------
| explained_variance | 0.183    |
| fps                | 237      |
| nupdates           | 1        |
| policy_entropy     | 1.42     |
| total_timesteps    | 40       |
| value_loss         | 5.71     |
---------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.12e+03 |
| explained_variance | 0.758     |
| fps                | 4688      |
| nupdates           | 100       |
| policy_entropy     | 1.42      |
| total_timesteps    | 4000      |
| value_loss         | 0.0528    |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.14e+03 |
| explained_variance | -27       |
| fps                | 4726      |
| nupdates           | 200       |
| policy_entropy     | 1.42      |
| total_timesteps    | 8000      |
| value_loss         | 0.147     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-1321.50 +/- 206.05
Episode length: 200.00 +/- 0.00
New best mean reward!
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.19e+03 |
| explained_variance | 0.241     |
| fps                | 3761      |
| nupdates           | 300       |
| policy_entropy     | 1.42      |
| total_timesteps    | 12000     |
| value_loss         | 0.0528    |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.18e+03 |
| explained_variance | -6.93     |
| fps                | 4094      |
| nupdates           | 400       |
| policy_entropy     | 1.42      |
| total_timesteps    | 16000     |
| value_loss         | 0.0589    |
----------------------------------
Eval num_timesteps=20000, episode_reward=-1610.31 +/- 336.28
Episode length: 200.00 +/- 0.00
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.17e+03 |
| explained_variance | 0.823     |
| fps                | 3894      |
| nupdates           | 500       |
| policy_entropy     | 1.42      |
| total_timesteps    | 20000     |
| value_loss         | 0.0202    |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.18e+03 |
| explained_variance | -5.51     |
| fps                | 4136      |
| nupdates           | 600       |
| policy_entropy     | 1.41      |
| total_timesteps    | 24000     |
| value_loss         | 0.0855    |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.19e+03 |
| explained_variance | 0.688     |
| fps                | 4338      |
| nupdates           | 700       |
| policy_entropy     | 1.42      |
| total_timesteps    | 28000     |
| value_loss         | 0.00971   |
----------------------------------
Eval num_timesteps=30000, episode_reward=-1181.14 +/- 396.27
Episode length: 200.00 +/- 0.00
New best mean reward!
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.17e+03 |
| explained_variance | -1.81     |
| fps                | 4172      |
| nupdates           | 800       |
| policy_entropy     | 1.42      |
| total_timesteps    | 32000     |
| value_loss         | 0.26      |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.17e+03 |
| explained_variance | 0.952     |
| fps                | 4327      |
| nupdates           | 900       |
| policy_entropy     | 1.42      |
| total_timesteps    | 36000     |
| value_loss         | 0.0013    |
----------------------------------
Eval num_timesteps=40000, episode_reward=-1108.49 +/- 202.82
Episode length: 200.00 +/- 0.00
New best mean reward!
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.19e+03 |
| explained_variance | -2.86     |
| fps                | 4195      |
| nupdates           | 1000      |
| policy_entropy     | 1.42      |
| total_timesteps    | 40000     |
| value_loss         | 0.226     |
----------------------------------
---------------------------------
| ep_len_mean        | 200      |
| ep_reward_mean     | -1.2e+03 |
| explained_variance | 0.884    |
| fps                | 4323     |
| nupdates           | 1100     |
| policy_entropy     | 1.41     |
| total_timesteps    | 44000    |
| value_loss         | 0.00414  |
---------------------------------
---------------------------------
| ep_len_mean        | 200      |
| ep_reward_mean     | -1.2e+03 |
| explained_variance | 0.385    |
| fps                | 4426     |
| nupdates           | 1200     |
| policy_entropy     | 1.41     |
| total_timesteps    | 48000    |
| value_loss         | 0.206    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-1282.65 +/- 228.28
Episode length: 200.00 +/- 0.00
---------------------------------
| ep_len_mean        | 200      |
| ep_reward_mean     | -1.2e+03 |
| explained_variance | 0.722    |
| fps                | 4311     |
| nupdates           | 1300     |
| policy_entropy     | 1.41     |
| total_timesteps    | 52000    |
| value_loss         | 0.00414  |
---------------------------------
---------------------------------
| ep_len_mean        | 200      |
| ep_reward_mean     | -1.2e+03 |
| explained_variance | -0.744   |
| fps                | 4406     |
| nupdates           | 1400     |
| policy_entropy     | 1.4      |
| total_timesteps    | 56000    |
| value_loss         | 0.261    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-1147.52 +/- 48.07
Episode length: 200.00 +/- 0.00
---------------------------------
| ep_len_mean        | 200      |
| ep_reward_mean     | -1.2e+03 |
| explained_variance | 0.942    |
| fps                | 4318     |
| nupdates           | 1500     |
| policy_entropy     | 1.4      |
| total_timesteps    | 60000    |
| value_loss         | 0.0012   |
---------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.22e+03 |
| explained_variance | 0.0388    |
| fps                | 4402      |
| nupdates           | 1600      |
| policy_entropy     | 1.4       |
| total_timesteps    | 64000     |
| value_loss         | 0.31      |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.22e+03 |
| explained_variance | 0.871     |
| fps                | 4480      |
| nupdates           | 1700      |
| policy_entropy     | 1.39      |
| total_timesteps    | 68000     |
| value_loss         | 0.00247   |
----------------------------------
Eval num_timesteps=70000, episode_reward=-1275.18 +/- 172.39
Episode length: 200.00 +/- 0.00
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.23e+03 |
| explained_variance | -0.138    |
| fps                | 4399      |
| nupdates           | 1800      |
| policy_entropy     | 1.39      |
| total_timesteps    | 72000     |
| value_loss         | 0.3       |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.24e+03 |
| explained_variance | 0.986     |
| fps                | 4450      |
| nupdates           | 1900      |
| policy_entropy     | 1.38      |
| total_timesteps    | 76000     |
| value_loss         | 0.000682  |
----------------------------------
Eval num_timesteps=80000, episode_reward=-1379.23 +/- 178.62
Episode length: 200.00 +/- 0.00
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.24e+03 |
| explained_variance | -1.79     |
| fps                | 4371      |
| nupdates           | 2000      |
| policy_entropy     | 1.37      |
| total_timesteps    | 80000     |
| value_loss         | 0.317     |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.22e+03 |
| explained_variance | 0.922     |
| fps                | 4435      |
| nupdates           | 2100      |
| policy_entropy     | 1.38      |
| total_timesteps    | 84000     |
| value_loss         | 0.00139   |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.24e+03 |
| explained_variance | -3.97     |
| fps                | 4492      |
| nupdates           | 2200      |
| policy_entropy     | 1.38      |
| total_timesteps    | 88000     |
| value_loss         | 0.522     |
----------------------------------
Eval num_timesteps=90000, episode_reward=-1278.57 +/- 60.91
Episode length: 200.00 +/- 0.00
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.24e+03 |
| explained_variance | 0.948     |
| fps                | 4424      |
| nupdates           | 2300      |
| policy_entropy     | 1.38      |
| total_timesteps    | 92000     |
| value_loss         | 0.000784  |
----------------------------------
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.25e+03 |
| explained_variance | -2.28     |
| fps                | 4479      |
| nupdates           | 2400      |
| policy_entropy     | 1.38      |
| total_timesteps    | 96000     |
| value_loss         | 0.361     |
----------------------------------
Eval num_timesteps=100000, episode_reward=-1377.52 +/- 165.80
Episode length: 200.00 +/- 0.00
----------------------------------
| ep_len_mean        | 200       |
| ep_reward_mean     | -1.25e+03 |
| explained_variance | 0.844     |
| fps                | 4412      |
| nupdates           | 2500      |
| policy_entropy     | 1.37      |
| total_timesteps    | 100000    |
| value_loss         | 0.00196   |
----------------------------------
Saving to logs/a2c/Pendulum-v0_Env_8_norm//a2c/Pendulum-v0_9
