WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/a2c/a2c.py:158: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/a2c/a2c.py:182: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/a2c/a2c.py:194: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('ent_coef', 0.001),
             ('gamma', 0.99),
             ('learning_rate', 0.002),
             ('lr_schedule', 'linear'),
             ('n_envs', 8),
             ('n_steps', 32),
             ('n_timesteps', 1000000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('vf_coef', 0.5)])
Using 8 environments
current_dir=/ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/pybullet_envs/bullet
Normalizing input and reward
Creating test environment
Normalization activated: {'norm_reward': False}
Log path: logs/a2c/Reacher2Dof-v0_1
options= 
options= 
options= 
options= 
options= 
options= 
options= 
options= 
---------------------------------
| explained_variance | -0.0238  |
| fps                | 432      |
| nupdates           | 1        |
| policy_entropy     | 2.84     |
| total_timesteps    | 256      |
| value_loss         | 7.14     |
---------------------------------
options= 
Eval num_timesteps=10000, episode_reward=2.80 +/- 9.68
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=5.71 +/- 7.28
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -16.9    |
| explained_variance | 0.145    |
| fps                | 2173     |
| nupdates           | 100      |
| policy_entropy     | 2.84     |
| total_timesteps    | 25600    |
| value_loss         | 0.792    |
---------------------------------
Eval num_timesteps=30000, episode_reward=3.85 +/- 8.27
Episode length: 150.00 +/- 0.00
Eval num_timesteps=40000, episode_reward=-1.75 +/- 11.24
Episode length: 150.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-0.44 +/- 10.38
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -14.6    |
| explained_variance | 0.234    |
| fps                | 2104     |
| nupdates           | 200      |
| policy_entropy     | 2.84     |
| total_timesteps    | 51200    |
| value_loss         | 0.675    |
---------------------------------
Eval num_timesteps=60000, episode_reward=4.82 +/- 12.26
Episode length: 150.00 +/- 0.00
Eval num_timesteps=70000, episode_reward=-3.64 +/- 9.71
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -14      |
| explained_variance | -0.133   |
| fps                | 2161     |
| nupdates           | 300      |
| policy_entropy     | 2.84     |
| total_timesteps    | 76800    |
| value_loss         | 1.06     |
---------------------------------
Eval num_timesteps=80000, episode_reward=-3.61 +/- 9.08
Episode length: 150.00 +/- 0.00
Eval num_timesteps=90000, episode_reward=-4.63 +/- 9.82
Episode length: 150.00 +/- 0.00
Eval num_timesteps=100000, episode_reward=-5.59 +/- 9.63
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -15.5    |
| explained_variance | 0.135    |
| fps                | 2132     |
| nupdates           | 400      |
| policy_entropy     | 2.84     |
| total_timesteps    | 102400   |
| value_loss         | 0.561    |
---------------------------------
Eval num_timesteps=110000, episode_reward=2.45 +/- 13.37
Episode length: 150.00 +/- 0.00
Eval num_timesteps=120000, episode_reward=1.37 +/- 10.38
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -15.3    |
| explained_variance | -0.0964  |
| fps                | 2167     |
| nupdates           | 500      |
| policy_entropy     | 2.84     |
| total_timesteps    | 128000   |
| value_loss         | 1.35     |
---------------------------------
Eval num_timesteps=130000, episode_reward=-3.58 +/- 10.25
Episode length: 150.00 +/- 0.00
Eval num_timesteps=140000, episode_reward=1.34 +/- 11.44
Episode length: 150.00 +/- 0.00
Eval num_timesteps=150000, episode_reward=-5.46 +/- 13.46
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -12.9    |
| explained_variance | 0.0743   |
| fps                | 2148     |
| nupdates           | 600      |
| policy_entropy     | 2.86     |
| total_timesteps    | 153600   |
| value_loss         | 1.09     |
---------------------------------
Eval num_timesteps=160000, episode_reward=-2.35 +/- 10.04
Episode length: 150.00 +/- 0.00
Eval num_timesteps=170000, episode_reward=3.69 +/- 12.55
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -11.2    |
| explained_variance | -0.217   |
| fps                | 2174     |
| nupdates           | 700      |
| policy_entropy     | 2.84     |
| total_timesteps    | 179200   |
| value_loss         | 0.471    |
---------------------------------
Eval num_timesteps=180000, episode_reward=-3.29 +/- 11.82
Episode length: 150.00 +/- 0.00
Eval num_timesteps=190000, episode_reward=0.05 +/- 7.17
Episode length: 150.00 +/- 0.00
Eval num_timesteps=200000, episode_reward=0.27 +/- 9.82
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.13    |
| explained_variance | 0.26     |
| fps                | 2164     |
| nupdates           | 800      |
| policy_entropy     | 2.81     |
| total_timesteps    | 204800   |
| value_loss         | 0.181    |
---------------------------------
Eval num_timesteps=210000, episode_reward=-0.70 +/- 10.69
Episode length: 150.00 +/- 0.00
Eval num_timesteps=220000, episode_reward=2.02 +/- 8.87
Episode length: 150.00 +/- 0.00
Eval num_timesteps=230000, episode_reward=3.31 +/- 11.72
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.03    |
| explained_variance | -0.067   |
| fps                | 2153     |
| nupdates           | 900      |
| policy_entropy     | 2.76     |
| total_timesteps    | 230400   |
| value_loss         | 0.146    |
---------------------------------
Eval num_timesteps=240000, episode_reward=-0.70 +/- 10.68
Episode length: 150.00 +/- 0.00
Eval num_timesteps=250000, episode_reward=4.31 +/- 13.27
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.85    |
| explained_variance | -0.277   |
| fps                | 2167     |
| nupdates           | 1000     |
| policy_entropy     | 2.76     |
| total_timesteps    | 256000   |
| value_loss         | 0.0641   |
---------------------------------
Eval num_timesteps=260000, episode_reward=4.37 +/- 6.67
Episode length: 150.00 +/- 0.00
Eval num_timesteps=270000, episode_reward=4.79 +/- 10.91
Episode length: 150.00 +/- 0.00
Eval num_timesteps=280000, episode_reward=-4.73 +/- 9.61
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.45    |
| explained_variance | 0.772    |
| fps                | 2154     |
| nupdates           | 1100     |
| policy_entropy     | 2.72     |
| total_timesteps    | 281600   |
| value_loss         | 0.0228   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-2.19 +/- 11.06
Episode length: 150.00 +/- 0.00
Eval num_timesteps=300000, episode_reward=0.95 +/- 9.47
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -7.83    |
| explained_variance | 0.000592 |
| fps                | 2167     |
| nupdates           | 1200     |
| policy_entropy     | 2.72     |
| total_timesteps    | 307200   |
| value_loss         | 0.354    |
---------------------------------
Eval num_timesteps=310000, episode_reward=3.76 +/- 10.21
Episode length: 150.00 +/- 0.00
Eval num_timesteps=320000, episode_reward=0.33 +/- 12.12
Episode length: 150.00 +/- 0.00
Eval num_timesteps=330000, episode_reward=-1.20 +/- 13.15
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.17    |
| explained_variance | 0.582    |
| fps                | 2160     |
| nupdates           | 1300     |
| policy_entropy     | 2.71     |
| total_timesteps    | 332800   |
| value_loss         | 0.0311   |
---------------------------------
Eval num_timesteps=340000, episode_reward=0.63 +/- 10.83
Episode length: 150.00 +/- 0.00
Eval num_timesteps=350000, episode_reward=-2.93 +/- 10.59
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -6.01    |
| explained_variance | 0.336    |
| fps                | 2172     |
| nupdates           | 1400     |
| policy_entropy     | 2.7      |
| total_timesteps    | 358400   |
| value_loss         | 0.0151   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1.59 +/- 10.07
Episode length: 150.00 +/- 0.00
Eval num_timesteps=370000, episode_reward=-4.50 +/- 9.06
Episode length: 150.00 +/- 0.00
Eval num_timesteps=380000, episode_reward=-2.97 +/- 12.49
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.41    |
| explained_variance | -0.265   |
| fps                | 2166     |
| nupdates           | 1500     |
| policy_entropy     | 2.68     |
| total_timesteps    | 384000   |
| value_loss         | 0.324    |
---------------------------------
Eval num_timesteps=390000, episode_reward=1.26 +/- 11.15
Episode length: 150.00 +/- 0.00
Eval num_timesteps=400000, episode_reward=-2.40 +/- 9.89
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.89    |
| explained_variance | 0.847    |
| fps                | 2176     |
| nupdates           | 1600     |
| policy_entropy     | 2.66     |
| total_timesteps    | 409600   |
| value_loss         | 0.00339  |
---------------------------------
Eval num_timesteps=410000, episode_reward=4.71 +/- 9.62
Episode length: 150.00 +/- 0.00
Eval num_timesteps=420000, episode_reward=6.08 +/- 7.89
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=430000, episode_reward=-2.83 +/- 10.54
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.54    |
| explained_variance | 0.794    |
| fps                | 2170     |
| nupdates           | 1700     |
| policy_entropy     | 2.64     |
| total_timesteps    | 435200   |
| value_loss         | 0.0163   |
---------------------------------
Eval num_timesteps=440000, episode_reward=5.81 +/- 10.79
Episode length: 150.00 +/- 0.00
Eval num_timesteps=450000, episode_reward=-0.45 +/- 10.20
Episode length: 150.00 +/- 0.00
Eval num_timesteps=460000, episode_reward=-0.79 +/- 11.94
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.79    |
| explained_variance | 0.234    |
| fps                | 2166     |
| nupdates           | 1800     |
| policy_entropy     | 2.62     |
| total_timesteps    | 460800   |
| value_loss         | 0.0865   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-2.71 +/- 7.16
Episode length: 150.00 +/- 0.00
Eval num_timesteps=480000, episode_reward=-2.62 +/- 11.17
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.85    |
| explained_variance | 0.778    |
| fps                | 2174     |
| nupdates           | 1900     |
| policy_entropy     | 2.6      |
| total_timesteps    | 486400   |
| value_loss         | 0.0112   |
---------------------------------
Eval num_timesteps=490000, episode_reward=3.67 +/- 11.01
Episode length: 150.00 +/- 0.00
Eval num_timesteps=500000, episode_reward=0.85 +/- 8.23
Episode length: 150.00 +/- 0.00
Eval num_timesteps=510000, episode_reward=4.34 +/- 9.08
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.88    |
| explained_variance | 0.626    |
| fps                | 2169     |
| nupdates           | 2000     |
| policy_entropy     | 2.58     |
| total_timesteps    | 512000   |
| value_loss         | 0.0105   |
---------------------------------
Eval num_timesteps=520000, episode_reward=7.17 +/- 5.70
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=530000, episode_reward=-1.55 +/- 4.89
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.56    |
| explained_variance | -3.81    |
| fps                | 2176     |
| nupdates           | 2100     |
| policy_entropy     | 2.57     |
| total_timesteps    | 537600   |
| value_loss         | 0.194    |
---------------------------------
Eval num_timesteps=540000, episode_reward=2.41 +/- 7.91
Episode length: 150.00 +/- 0.00
Eval num_timesteps=550000, episode_reward=-2.94 +/- 13.57
Episode length: 150.00 +/- 0.00
Eval num_timesteps=560000, episode_reward=2.12 +/- 8.03
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.3     |
| explained_variance | 0.87     |
| fps                | 2171     |
| nupdates           | 2200     |
| policy_entropy     | 2.57     |
| total_timesteps    | 563200   |
| value_loss         | 0.00717  |
---------------------------------
Eval num_timesteps=570000, episode_reward=2.52 +/- 7.20
Episode length: 150.00 +/- 0.00
Eval num_timesteps=580000, episode_reward=3.58 +/- 8.30
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.4     |
| explained_variance | 0.128    |
| fps                | 2178     |
| nupdates           | 2300     |
| policy_entropy     | 2.58     |
| total_timesteps    | 588800   |
| value_loss         | 0.157    |
---------------------------------
Eval num_timesteps=590000, episode_reward=0.40 +/- 10.13
Episode length: 150.00 +/- 0.00
Eval num_timesteps=600000, episode_reward=1.63 +/- 11.30
Episode length: 150.00 +/- 0.00
Eval num_timesteps=610000, episode_reward=0.76 +/- 9.81
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.26    |
| explained_variance | -1.76    |
| fps                | 2174     |
| nupdates           | 2400     |
| policy_entropy     | 2.59     |
| total_timesteps    | 614400   |
| value_loss         | 0.119    |
---------------------------------
Eval num_timesteps=620000, episode_reward=4.43 +/- 8.41
Episode length: 150.00 +/- 0.00
Eval num_timesteps=630000, episode_reward=1.32 +/- 11.70
Episode length: 150.00 +/- 0.00
Eval num_timesteps=640000, episode_reward=0.99 +/- 11.08
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.59    |
| explained_variance | -0.0066  |
| fps                | 2171     |
| nupdates           | 2500     |
| policy_entropy     | 2.57     |
| total_timesteps    | 640000   |
| value_loss         | 0.201    |
---------------------------------
Eval num_timesteps=650000, episode_reward=-4.00 +/- 9.47
Episode length: 150.00 +/- 0.00
Eval num_timesteps=660000, episode_reward=-1.96 +/- 9.82
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.61    |
| explained_variance | 0.852    |
| fps                | 2176     |
| nupdates           | 2600     |
| policy_entropy     | 2.56     |
| total_timesteps    | 665600   |
| value_loss         | 0.0102   |
---------------------------------
Eval num_timesteps=670000, episode_reward=2.37 +/- 8.58
Episode length: 150.00 +/- 0.00
Eval num_timesteps=680000, episode_reward=-1.15 +/- 6.81
Episode length: 150.00 +/- 0.00
Eval num_timesteps=690000, episode_reward=0.65 +/- 7.98
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.86    |
| explained_variance | -3.52    |
| fps                | 2172     |
| nupdates           | 2700     |
| policy_entropy     | 2.56     |
| total_timesteps    | 691200   |
| value_loss         | 0.203    |
---------------------------------
Eval num_timesteps=700000, episode_reward=0.67 +/- 9.40
Episode length: 150.00 +/- 0.00
Eval num_timesteps=710000, episode_reward=-0.93 +/- 8.33
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -5       |
| explained_variance | 0.485    |
| fps                | 2176     |
| nupdates           | 2800     |
| policy_entropy     | 2.55     |
| total_timesteps    | 716800   |
| value_loss         | 0.00822  |
---------------------------------
Eval num_timesteps=720000, episode_reward=2.44 +/- 9.58
Episode length: 150.00 +/- 0.00
Eval num_timesteps=730000, episode_reward=-0.24 +/- 7.86
Episode length: 150.00 +/- 0.00
Eval num_timesteps=740000, episode_reward=-4.93 +/- 8.93
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.86    |
| explained_variance | 0.916    |
| fps                | 2173     |
| nupdates           | 2900     |
| policy_entropy     | 2.55     |
| total_timesteps    | 742400   |
| value_loss         | 0.00316  |
---------------------------------
Eval num_timesteps=750000, episode_reward=4.17 +/- 5.49
Episode length: 150.00 +/- 0.00
Eval num_timesteps=760000, episode_reward=3.99 +/- 6.49
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.88    |
| explained_variance | -1.25    |
| fps                | 2177     |
| nupdates           | 3000     |
| policy_entropy     | 2.54     |
| total_timesteps    | 768000   |
| value_loss         | 0.162    |
---------------------------------
Eval num_timesteps=770000, episode_reward=-1.39 +/- 10.35
Episode length: 150.00 +/- 0.00
Eval num_timesteps=780000, episode_reward=-5.25 +/- 6.46
Episode length: 150.00 +/- 0.00
Eval num_timesteps=790000, episode_reward=0.76 +/- 9.51
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.38    |
| explained_variance | 0.483    |
| fps                | 2173     |
| nupdates           | 3100     |
| policy_entropy     | 2.54     |
| total_timesteps    | 793600   |
| value_loss         | 0.0189   |
---------------------------------
Eval num_timesteps=800000, episode_reward=6.46 +/- 6.10
Episode length: 150.00 +/- 0.00
Eval num_timesteps=810000, episode_reward=5.50 +/- 6.31
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.57    |
| explained_variance | 0.833    |
| fps                | 2178     |
| nupdates           | 3200     |
| policy_entropy     | 2.54     |
| total_timesteps    | 819200   |
| value_loss         | 0.00911  |
---------------------------------
Eval num_timesteps=820000, episode_reward=-1.63 +/- 11.63
Episode length: 150.00 +/- 0.00
Eval num_timesteps=830000, episode_reward=3.63 +/- 11.04
Episode length: 150.00 +/- 0.00
Eval num_timesteps=840000, episode_reward=-5.41 +/- 14.84
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.97    |
| explained_variance | -0.534   |
| fps                | 2174     |
| nupdates           | 3300     |
| policy_entropy     | 2.54     |
| total_timesteps    | 844800   |
| value_loss         | 0.193    |
---------------------------------
Eval num_timesteps=850000, episode_reward=0.78 +/- 12.92
Episode length: 150.00 +/- 0.00
Eval num_timesteps=860000, episode_reward=-3.48 +/- 7.73
Episode length: 150.00 +/- 0.00
Eval num_timesteps=870000, episode_reward=-2.07 +/- 11.67
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.91    |
| explained_variance | 0.794    |
| fps                | 2171     |
| nupdates           | 3400     |
| policy_entropy     | 2.54     |
| total_timesteps    | 870400   |
| value_loss         | 0.00713  |
---------------------------------
Eval num_timesteps=880000, episode_reward=0.48 +/- 12.30
Episode length: 150.00 +/- 0.00
Eval num_timesteps=890000, episode_reward=1.88 +/- 9.12
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.21    |
| explained_variance | 0.732    |
| fps                | 2174     |
| nupdates           | 3500     |
| policy_entropy     | 2.54     |
| total_timesteps    | 896000   |
| value_loss         | 0.0145   |
---------------------------------
Eval num_timesteps=900000, episode_reward=-1.44 +/- 8.58
Episode length: 150.00 +/- 0.00
Eval num_timesteps=910000, episode_reward=0.08 +/- 8.10
Episode length: 150.00 +/- 0.00
Eval num_timesteps=920000, episode_reward=-1.21 +/- 10.50
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.42    |
| explained_variance | -1       |
| fps                | 2172     |
| nupdates           | 3600     |
| policy_entropy     | 2.54     |
| total_timesteps    | 921600   |
| value_loss         | 0.0967   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2.24 +/- 8.78
Episode length: 150.00 +/- 0.00
Eval num_timesteps=940000, episode_reward=0.58 +/- 10.28
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.48    |
| explained_variance | 0.642    |
| fps                | 2175     |
| nupdates           | 3700     |
| policy_entropy     | 2.54     |
| total_timesteps    | 947200   |
| value_loss         | 0.0126   |
---------------------------------
Eval num_timesteps=950000, episode_reward=4.88 +/- 7.71
Episode length: 150.00 +/- 0.00
Eval num_timesteps=960000, episode_reward=-2.72 +/- 7.70
Episode length: 150.00 +/- 0.00
Eval num_timesteps=970000, episode_reward=1.35 +/- 12.35
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.74    |
| explained_variance | 0.646    |
| fps                | 2173     |
| nupdates           | 3800     |
| policy_entropy     | 2.54     |
| total_timesteps    | 972800   |
| value_loss         | 0.00384  |
---------------------------------
Eval num_timesteps=980000, episode_reward=2.03 +/- 7.68
Episode length: 150.00 +/- 0.00
Eval num_timesteps=990000, episode_reward=-4.40 +/- 12.77
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.32    |
| explained_variance | -3.66    |
| fps                | 2176     |
| nupdates           | 3900     |
| policy_entropy     | 2.54     |
| total_timesteps    | 998400   |
| value_loss         | 0.195    |
---------------------------------
Saving to logs/a2c/Reacher2Dof-v0_1
pybullet build time: Mar 20 2020 20:01:46
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
