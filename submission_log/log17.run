WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('cliprange', 0.2),
             ('ent_coef', 0.0),
             ('gamma', 0.99),
             ('lam', 0.95),
             ('learning_rate', 0.00025),
             ('n_envs', 8),
             ('n_steps', 2048),
             ('n_timesteps', 1000000.0),
             ('nminibatches', 32),
             ('noptepochs', 10),
             ('normalize', True),
             ('policy', 'MlpPolicy')])
Using 8 environments
current_dir=/ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/pybullet_envs/bullet
Normalizing input and reward
Creating test environment
Normalization activated: {'norm_reward': False}
Log path: logs/ppo2/Reacher2Dof-v0_1
options= 
options= 
options= 
options= 
options= 
options= 
options= 
options= 
options= 
Eval num_timesteps=10000, episode_reward=4.93 +/- 7.63
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0036822534  |
| clipfrac           | 0.03571167    |
| ep_len_mean        | 150           |
| ep_reward_mean     | -16           |
| explained_variance | 0.0505        |
| fps                | 1981          |
| n_updates          | 1             |
| policy_entropy     | 2.822146      |
| policy_loss        | -0.0032941948 |
| serial_timesteps   | 2048          |
| time_elapsed       | 2.43e-05      |
| total_timesteps    | 16384         |
| value_loss         | 0.26049176    |
--------------------------------------
Eval num_timesteps=20000, episode_reward=0.97 +/- 10.32
Episode length: 150.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=1.07 +/- 12.81
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.004903745   |
| clipfrac           | 0.053533934   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -14.4         |
| explained_variance | 0.126         |
| fps                | 1831          |
| n_updates          | 2             |
| policy_entropy     | 2.804554      |
| policy_loss        | -0.0046529393 |
| serial_timesteps   | 4096          |
| time_elapsed       | 8.27          |
| total_timesteps    | 32768         |
| value_loss         | 0.18905064    |
--------------------------------------
Eval num_timesteps=40000, episode_reward=1.37 +/- 10.54
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0059726047  |
| clipfrac           | 0.074645996   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -11.3         |
| explained_variance | 0.225         |
| fps                | 2180          |
| n_updates          | 3             |
| policy_entropy     | 2.7926505     |
| policy_loss        | -0.0058647995 |
| serial_timesteps   | 6144          |
| time_elapsed       | 17.2          |
| total_timesteps    | 49152         |
| value_loss         | 0.12732962    |
--------------------------------------
Eval num_timesteps=50000, episode_reward=1.66 +/- 13.38
Episode length: 150.00 +/- 0.00
Eval num_timesteps=60000, episode_reward=7.03 +/- 6.78
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.005666406   |
| clipfrac           | 0.06970825    |
| ep_len_mean        | 150           |
| ep_reward_mean     | -10.5         |
| explained_variance | 0.419         |
| fps                | 1919          |
| n_updates          | 4             |
| policy_entropy     | 2.7789774     |
| policy_loss        | -0.0054269643 |
| serial_timesteps   | 8192          |
| time_elapsed       | 24.7          |
| total_timesteps    | 65536         |
| value_loss         | 0.07967723    |
--------------------------------------
Eval num_timesteps=70000, episode_reward=-2.28 +/- 7.06
Episode length: 150.00 +/- 0.00
Eval num_timesteps=80000, episode_reward=-1.88 +/- 10.67
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0034639314  |
| clipfrac           | 0.031384278   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -9.3          |
| explained_variance | 0.573         |
| fps                | 1842          |
| n_updates          | 5             |
| policy_entropy     | 2.7650847     |
| policy_loss        | -0.0032481994 |
| serial_timesteps   | 10240         |
| time_elapsed       | 33.3          |
| total_timesteps    | 81920         |
| value_loss         | 0.049590215   |
--------------------------------------
Eval num_timesteps=90000, episode_reward=-6.55 +/- 9.86
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0038111764 |
| clipfrac           | 0.03369751   |
| ep_len_mean        | 150          |
| ep_reward_mean     | -8.82        |
| explained_variance | 0.662        |
| fps                | 2172         |
| n_updates          | 6            |
| policy_entropy     | 2.7440658    |
| policy_loss        | -0.00349821  |
| serial_timesteps   | 12288        |
| time_elapsed       | 42.2         |
| total_timesteps    | 98304        |
| value_loss         | 0.041944843  |
-------------------------------------
Eval num_timesteps=100000, episode_reward=-1.35 +/- 7.48
Episode length: 150.00 +/- 0.00
Eval num_timesteps=110000, episode_reward=2.53 +/- 12.95
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0037477382  |
| clipfrac           | 0.034680177   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -8.78         |
| explained_variance | 0.739         |
| fps                | 1827          |
| n_updates          | 7             |
| policy_entropy     | 2.717716      |
| policy_loss        | -0.0034945582 |
| serial_timesteps   | 14336         |
| time_elapsed       | 49.7          |
| total_timesteps    | 114688        |
| value_loss         | 0.032402616   |
--------------------------------------
Eval num_timesteps=120000, episode_reward=-1.64 +/- 9.05
Episode length: 150.00 +/- 0.00
Eval num_timesteps=130000, episode_reward=0.67 +/- 9.08
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0028278325  |
| clipfrac           | 0.020440673   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -7.87         |
| explained_variance | 0.78          |
| fps                | 1848          |
| n_updates          | 8             |
| policy_entropy     | 2.692194      |
| policy_loss        | -0.0026754062 |
| serial_timesteps   | 16384         |
| time_elapsed       | 58.7          |
| total_timesteps    | 131072        |
| value_loss         | 0.02550978    |
--------------------------------------
Eval num_timesteps=140000, episode_reward=-0.18 +/- 10.33
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.003428944   |
| clipfrac           | 0.026483154   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -6.54         |
| explained_variance | 0.819         |
| fps                | 2199          |
| n_updates          | 9             |
| policy_entropy     | 2.6686893     |
| policy_loss        | -0.0029312177 |
| serial_timesteps   | 18432         |
| time_elapsed       | 67.5          |
| total_timesteps    | 147456        |
| value_loss         | 0.019348215   |
--------------------------------------
Eval num_timesteps=150000, episode_reward=-3.38 +/- 9.65
Episode length: 150.00 +/- 0.00
Eval num_timesteps=160000, episode_reward=-0.96 +/- 10.29
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0035048742  |
| clipfrac           | 0.029803466   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -7.14         |
| explained_variance | 0.835         |
| fps                | 1875          |
| n_updates          | 10            |
| policy_entropy     | 2.6384008     |
| policy_loss        | -0.0034530922 |
| serial_timesteps   | 20480         |
| time_elapsed       | 75            |
| total_timesteps    | 163840        |
| value_loss         | 0.018110521   |
--------------------------------------
Eval num_timesteps=170000, episode_reward=4.23 +/- 11.36
Episode length: 150.00 +/- 0.00
Eval num_timesteps=180000, episode_reward=-4.54 +/- 11.28
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.004326921  |
| clipfrac           | 0.038964845  |
| ep_len_mean        | 150          |
| ep_reward_mean     | -5.5         |
| explained_variance | 0.868        |
| fps                | 1925         |
| n_updates          | 11           |
| policy_entropy     | 2.6035423    |
| policy_loss        | -0.003650108 |
| serial_timesteps   | 22528        |
| time_elapsed       | 83.7         |
| total_timesteps    | 180224       |
| value_loss         | 0.017292038  |
-------------------------------------
Eval num_timesteps=190000, episode_reward=0.43 +/- 6.63
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0031963375 |
| clipfrac           | 0.025628662  |
| ep_len_mean        | 150          |
| ep_reward_mean     | -5.73        |
| explained_variance | 0.885        |
| fps                | 2197         |
| n_updates          | 12           |
| policy_entropy     | 2.5837834    |
| policy_loss        | -0.002487048 |
| serial_timesteps   | 24576        |
| time_elapsed       | 92.2         |
| total_timesteps    | 196608       |
| value_loss         | 0.015760256  |
-------------------------------------
Eval num_timesteps=200000, episode_reward=3.73 +/- 8.93
Episode length: 150.00 +/- 0.00
Eval num_timesteps=210000, episode_reward=0.12 +/- 12.19
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.003680389  |
| clipfrac           | 0.03057251   |
| ep_len_mean        | 150          |
| ep_reward_mean     | -4.96        |
| explained_variance | 0.825        |
| fps                | 1921         |
| n_updates          | 13           |
| policy_entropy     | 2.5550952    |
| policy_loss        | -0.003237604 |
| serial_timesteps   | 26624        |
| time_elapsed       | 99.7         |
| total_timesteps    | 212992       |
| value_loss         | 0.024756808  |
-------------------------------------
Eval num_timesteps=220000, episode_reward=4.79 +/- 9.74
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0035447467  |
| clipfrac           | 0.03234253    |
| ep_len_mean        | 150           |
| ep_reward_mean     | -3.84         |
| explained_variance | 0.835         |
| fps                | 2325          |
| n_updates          | 14            |
| policy_entropy     | 2.5357056     |
| policy_loss        | -0.0037263583 |
| serial_timesteps   | 28672         |
| time_elapsed       | 108           |
| total_timesteps    | 229376        |
| value_loss         | 0.023153646   |
--------------------------------------
Eval num_timesteps=230000, episode_reward=3.77 +/- 9.70
Episode length: 150.00 +/- 0.00
Eval num_timesteps=240000, episode_reward=-0.11 +/- 14.82
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0030282913  |
| clipfrac           | 0.02368164    |
| ep_len_mean        | 150           |
| ep_reward_mean     | -4.42         |
| explained_variance | 0.867         |
| fps                | 1853          |
| n_updates          | 15            |
| policy_entropy     | 2.524295      |
| policy_loss        | -0.0032018132 |
| serial_timesteps   | 30720         |
| time_elapsed       | 115           |
| total_timesteps    | 245760        |
| value_loss         | 0.020172622   |
--------------------------------------
Eval num_timesteps=250000, episode_reward=6.97 +/- 12.27
Episode length: 150.00 +/- 0.00
Eval num_timesteps=260000, episode_reward=5.61 +/- 7.95
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.004185465   |
| clipfrac           | 0.040368654   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -4.12         |
| explained_variance | 0.874         |
| fps                | 1906          |
| n_updates          | 16            |
| policy_entropy     | 2.4982104     |
| policy_loss        | -0.0043485733 |
| serial_timesteps   | 32768         |
| time_elapsed       | 124           |
| total_timesteps    | 262144        |
| value_loss         | 0.022531439   |
--------------------------------------
Eval num_timesteps=270000, episode_reward=4.93 +/- 9.26
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0034623728 |
| clipfrac           | 0.031091308  |
| ep_len_mean        | 150          |
| ep_reward_mean     | -3.79        |
| explained_variance | 0.843        |
| fps                | 2099         |
| n_updates          | 17           |
| policy_entropy     | 2.4709601    |
| policy_loss        | -0.004113302 |
| serial_timesteps   | 34816        |
| time_elapsed       | 133          |
| total_timesteps    | 278528       |
| value_loss         | 0.02043932   |
-------------------------------------
Eval num_timesteps=280000, episode_reward=-0.42 +/- 8.71
Episode length: 150.00 +/- 0.00
Eval num_timesteps=290000, episode_reward=1.51 +/- 10.06
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.003967085   |
| clipfrac           | 0.03809204    |
| ep_len_mean        | 150           |
| ep_reward_mean     | -2.91         |
| explained_variance | 0.88          |
| fps                | 1748          |
| n_updates          | 18            |
| policy_entropy     | 2.4498117     |
| policy_loss        | -0.0037406366 |
| serial_timesteps   | 36864         |
| time_elapsed       | 141           |
| total_timesteps    | 294912        |
| value_loss         | 0.016961236   |
--------------------------------------
Eval num_timesteps=300000, episode_reward=1.30 +/- 8.44
Episode length: 150.00 +/- 0.00
Eval num_timesteps=310000, episode_reward=5.54 +/- 11.06
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0047981082  |
| clipfrac           | 0.046209715   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -4.52         |
| explained_variance | 0.864         |
| fps                | 1854          |
| n_updates          | 19            |
| policy_entropy     | 2.4180424     |
| policy_loss        | -0.0044746855 |
| serial_timesteps   | 38912         |
| time_elapsed       | 150           |
| total_timesteps    | 311296        |
| value_loss         | 0.019074317   |
--------------------------------------
Eval num_timesteps=320000, episode_reward=3.26 +/- 12.85
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0038037158 |
| clipfrac           | 0.03354492   |
| ep_len_mean        | 150          |
| ep_reward_mean     | -5           |
| explained_variance | 0.877        |
| fps                | 2133         |
| n_updates          | 20           |
| policy_entropy     | 2.3852715    |
| policy_loss        | -0.003592115 |
| serial_timesteps   | 40960        |
| time_elapsed       | 159          |
| total_timesteps    | 327680       |
| value_loss         | 0.017803099  |
-------------------------------------
Eval num_timesteps=330000, episode_reward=4.62 +/- 13.18
Episode length: 150.00 +/- 0.00
Eval num_timesteps=340000, episode_reward=4.64 +/- 11.89
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0048406417  |
| clipfrac           | 0.051031493   |
| ep_len_mean        | 150           |
| ep_reward_mean     | -2.02         |
| explained_variance | 0.886         |
| fps                | 1955          |
| n_updates          | 21            |
| policy_entropy     | 2.352276      |
| policy_loss        | -0.0053681084 |
| serial_timesteps   | 43008         |
| time_elapsed       | 166           |
| total_timesteps    | 344064        |
| value_loss         | 0.02122985    |
--------------------------------------
Eval num_timesteps=350000, episode_reward=5.78 +/- 6.66
Episode length: 150.00 +/- 0.00
Eval num_timesteps=360000, episode_reward=1.50 +/- 12.60
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.004219329  |
| clipfrac           | 0.041668702  |
| ep_len_mean        | 150          |
| ep_reward_mean     | -3.39        |
| explained_variance | 0.895        |
| fps                | 1850         |
| n_updates          | 22           |
| policy_entropy     | 2.326772     |
| policy_loss        | -0.004499402 |
| serial_timesteps   | 45056        |
| time_elapsed       | 175          |
| total_timesteps    | 360448       |
| value_loss         | 0.017210916  |
-------------------------------------
Eval num_timesteps=370000, episode_reward=0.63 +/- 9.95
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.003957854  |
| clipfrac           | 0.036999512  |
| ep_len_mean        | 150          |
| ep_reward_mean     | -3.72        |
| explained_variance | 0.88         |
| fps                | 2267         |
| n_updates          | 23           |
| policy_entropy     | 2.3085425    |
| policy_loss        | -0.004119705 |
| serial_timesteps   | 47104        |
| time_elapsed       | 184          |
| total_timesteps    | 376832       |
| value_loss         | 0.02050994   |
-------------------------------------
Eval num_timesteps=380000, episode_reward=3.75 +/- 5.55
Episode length: 150.00 +/- 0.00
Eval num_timesteps=390000, episode_reward=4.17 +/- 11.25
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0045653265  |
| clipfrac           | 0.04562378    |
| ep_len_mean        | 150           |
| ep_reward_mean     | -2.3          |
| explained_variance | 0.887         |
| fps                | 1856          |
| n_updates          | 24            |
| policy_entropy     | 2.2875893     |
| policy_loss        | -0.0047949636 |
| serial_timesteps   | 49152         |
| time_elapsed       | 191           |
| total_timesteps    | 393216        |
| value_loss         | 0.017021554   |
--------------------------------------
Eval num_timesteps=400000, episode_reward=4.41 +/- 12.20
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.005050455  |
| clipfrac           | 0.051214598  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 0.478        |
| explained_variance | 0.896        |
| fps                | 2266         |
| n_updates          | 25           |
| policy_entropy     | 2.254722     |
| policy_loss        | -0.005784437 |
| serial_timesteps   | 51200        |
| time_elapsed       | 200          |
| total_timesteps    | 409600       |
| value_loss         | 0.018496275  |
-------------------------------------
Eval num_timesteps=410000, episode_reward=10.75 +/- 14.07
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=420000, episode_reward=12.82 +/- 8.29
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0049662655  |
| clipfrac           | 0.052227784   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 0.344         |
| explained_variance | 0.888         |
| fps                | 1891          |
| n_updates          | 26            |
| policy_entropy     | 2.2172294     |
| policy_loss        | -0.0061175684 |
| serial_timesteps   | 53248         |
| time_elapsed       | 207           |
| total_timesteps    | 425984        |
| value_loss         | 0.02226385    |
--------------------------------------
Eval num_timesteps=430000, episode_reward=0.99 +/- 12.10
Episode length: 150.00 +/- 0.00
Eval num_timesteps=440000, episode_reward=15.21 +/- 10.09
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0051032295  |
| clipfrac           | 0.05531006    |
| ep_len_mean        | 150           |
| ep_reward_mean     | 1.64          |
| explained_variance | 0.907         |
| fps                | 1850          |
| n_updates          | 27            |
| policy_entropy     | 2.18184       |
| policy_loss        | -0.0066888677 |
| serial_timesteps   | 55296         |
| time_elapsed       | 216           |
| total_timesteps    | 442368        |
| value_loss         | 0.02186684    |
--------------------------------------
Eval num_timesteps=450000, episode_reward=15.46 +/- 14.01
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0044308347  |
| clipfrac           | 0.04541626    |
| ep_len_mean        | 150           |
| ep_reward_mean     | 2.09          |
| explained_variance | 0.902         |
| fps                | 2274          |
| n_updates          | 28            |
| policy_entropy     | 2.158647      |
| policy_loss        | -0.0054393606 |
| serial_timesteps   | 57344         |
| time_elapsed       | 224           |
| total_timesteps    | 458752        |
| value_loss         | 0.02450971    |
--------------------------------------
Eval num_timesteps=460000, episode_reward=16.12 +/- 9.84
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=470000, episode_reward=1.38 +/- 10.55
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0044903895 |
| clipfrac           | 0.046209715  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 2.41         |
| explained_variance | 0.913        |
| fps                | 1931         |
| n_updates          | 29           |
| policy_entropy     | 2.137936     |
| policy_loss        | -0.005245122 |
| serial_timesteps   | 59392        |
| time_elapsed       | 232          |
| total_timesteps    | 475136       |
| value_loss         | 0.018613238  |
-------------------------------------
Eval num_timesteps=480000, episode_reward=8.59 +/- 9.06
Episode length: 150.00 +/- 0.00
Eval num_timesteps=490000, episode_reward=12.76 +/- 7.01
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.005213875   |
| clipfrac           | 0.053704835   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 3.14          |
| explained_variance | 0.901         |
| fps                | 1752          |
| n_updates          | 30            |
| policy_entropy     | 2.106157      |
| policy_loss        | -0.0061777383 |
| serial_timesteps   | 61440         |
| time_elapsed       | 240           |
| total_timesteps    | 491520        |
| value_loss         | 0.022581223   |
--------------------------------------
Eval num_timesteps=500000, episode_reward=11.88 +/- 13.79
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.004049075   |
| clipfrac           | 0.039355468   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 5.77          |
| explained_variance | 0.927         |
| fps                | 2212          |
| n_updates          | 31            |
| policy_entropy     | 2.0781431     |
| policy_loss        | -0.0045509106 |
| serial_timesteps   | 63488         |
| time_elapsed       | 249           |
| total_timesteps    | 507904        |
| value_loss         | 0.020947427   |
--------------------------------------
Eval num_timesteps=510000, episode_reward=10.16 +/- 12.99
Episode length: 150.00 +/- 0.00
Eval num_timesteps=520000, episode_reward=13.21 +/- 11.92
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.004635484   |
| clipfrac           | 0.047888182   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 4.26          |
| explained_variance | 0.925         |
| fps                | 1963          |
| n_updates          | 32            |
| policy_entropy     | 2.0478077     |
| policy_loss        | -0.0058255987 |
| serial_timesteps   | 65536         |
| time_elapsed       | 257           |
| total_timesteps    | 524288        |
| value_loss         | 0.018656334   |
--------------------------------------
Eval num_timesteps=530000, episode_reward=9.77 +/- 9.20
Episode length: 150.00 +/- 0.00
Eval num_timesteps=540000, episode_reward=12.52 +/- 11.75
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.00488717    |
| clipfrac           | 0.048754882   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 7.15          |
| explained_variance | 0.929         |
| fps                | 1946          |
| n_updates          | 33            |
| policy_entropy     | 2.0083745     |
| policy_loss        | -0.0056144535 |
| serial_timesteps   | 67584         |
| time_elapsed       | 265           |
| total_timesteps    | 540672        |
| value_loss         | 0.019850615   |
--------------------------------------
Eval num_timesteps=550000, episode_reward=11.28 +/- 7.51
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0051495517 |
| clipfrac           | 0.055566408  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 6.97         |
| explained_variance | 0.927        |
| fps                | 2289         |
| n_updates          | 34           |
| policy_entropy     | 1.9634931    |
| policy_loss        | -0.006043386 |
| serial_timesteps   | 69632        |
| time_elapsed       | 274          |
| total_timesteps    | 557056       |
| value_loss         | 0.019708823  |
-------------------------------------
Eval num_timesteps=560000, episode_reward=14.51 +/- 11.49
Episode length: 150.00 +/- 0.00
Eval num_timesteps=570000, episode_reward=17.80 +/- 8.79
Episode length: 150.00 +/- 0.00
New best mean reward!
-------------------------------------
| approxkl           | 0.0050398735 |
| clipfrac           | 0.053491212  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 9.29         |
| explained_variance | 0.937        |
| fps                | 1934         |
| n_updates          | 35           |
| policy_entropy     | 1.9274002    |
| policy_loss        | -0.005569521 |
| serial_timesteps   | 71680        |
| time_elapsed       | 281          |
| total_timesteps    | 573440       |
| value_loss         | 0.01623803   |
-------------------------------------
Eval num_timesteps=580000, episode_reward=14.56 +/- 9.47
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0050459937 |
| clipfrac           | 0.05343628   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 9.24         |
| explained_variance | 0.936        |
| fps                | 2314         |
| n_updates          | 36           |
| policy_entropy     | 1.8889351    |
| policy_loss        | -0.006018044 |
| serial_timesteps   | 73728        |
| time_elapsed       | 289          |
| total_timesteps    | 589824       |
| value_loss         | 0.014667323  |
-------------------------------------
Eval num_timesteps=590000, episode_reward=12.44 +/- 9.86
Episode length: 150.00 +/- 0.00
Eval num_timesteps=600000, episode_reward=15.63 +/- 8.31
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.005078749  |
| clipfrac           | 0.051416017  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 7.42         |
| explained_variance | 0.944        |
| fps                | 1978         |
| n_updates          | 37           |
| policy_entropy     | 1.8545868    |
| policy_loss        | -0.005387687 |
| serial_timesteps   | 75776        |
| time_elapsed       | 296          |
| total_timesteps    | 606208       |
| value_loss         | 0.012559392  |
-------------------------------------
Eval num_timesteps=610000, episode_reward=10.73 +/- 11.23
Episode length: 150.00 +/- 0.00
Eval num_timesteps=620000, episode_reward=14.47 +/- 8.11
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.004937244   |
| clipfrac           | 0.053393554   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 10.1          |
| explained_variance | 0.948         |
| fps                | 1861          |
| n_updates          | 38            |
| policy_entropy     | 1.816295      |
| policy_loss        | -0.0066158203 |
| serial_timesteps   | 77824         |
| time_elapsed       | 305           |
| total_timesteps    | 622592        |
| value_loss         | 0.010348926   |
--------------------------------------
Eval num_timesteps=630000, episode_reward=12.58 +/- 9.74
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.004931892   |
| clipfrac           | 0.055737305   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 9.19          |
| explained_variance | 0.951         |
| fps                | 2330          |
| n_updates          | 39            |
| policy_entropy     | 1.7834728     |
| policy_loss        | -0.0064140586 |
| serial_timesteps   | 79872         |
| time_elapsed       | 313           |
| total_timesteps    | 638976        |
| value_loss         | 0.010579939   |
--------------------------------------
Eval num_timesteps=640000, episode_reward=19.12 +/- 7.83
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=650000, episode_reward=15.15 +/- 6.17
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0051438566  |
| clipfrac           | 0.054711916   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 7.96          |
| explained_variance | 0.933         |
| fps                | 1936          |
| n_updates          | 40            |
| policy_entropy     | 1.7544628     |
| policy_loss        | -0.0051138056 |
| serial_timesteps   | 81920         |
| time_elapsed       | 320           |
| total_timesteps    | 655360        |
| value_loss         | 0.01252806    |
--------------------------------------
Eval num_timesteps=660000, episode_reward=12.94 +/- 6.91
Episode length: 150.00 +/- 0.00
Eval num_timesteps=670000, episode_reward=18.97 +/- 8.58
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0051963856 |
| clipfrac           | 0.054852296  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 9.64         |
| explained_variance | 0.961        |
| fps                | 1792         |
| n_updates          | 41           |
| policy_entropy     | 1.7209885    |
| policy_loss        | -0.006582316 |
| serial_timesteps   | 83968        |
| time_elapsed       | 329          |
| total_timesteps    | 671744       |
| value_loss         | 0.007707256  |
-------------------------------------
Eval num_timesteps=680000, episode_reward=14.61 +/- 11.75
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0049368637 |
| clipfrac           | 0.051855467  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 9.38         |
| explained_variance | 0.95         |
| fps                | 2239         |
| n_updates          | 42           |
| policy_entropy     | 1.6818546    |
| policy_loss        | -0.006240496 |
| serial_timesteps   | 86016        |
| time_elapsed       | 338          |
| total_timesteps    | 688128       |
| value_loss         | 0.010407599  |
-------------------------------------
Eval num_timesteps=690000, episode_reward=16.16 +/- 12.42
Episode length: 150.00 +/- 0.00
Eval num_timesteps=700000, episode_reward=16.06 +/- 7.44
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.005855105   |
| clipfrac           | 0.0668457     |
| ep_len_mean        | 150           |
| ep_reward_mean     | 9.44          |
| explained_variance | 0.956         |
| fps                | 1944          |
| n_updates          | 43            |
| policy_entropy     | 1.6450924     |
| policy_loss        | -0.0064839893 |
| serial_timesteps   | 88064         |
| time_elapsed       | 345           |
| total_timesteps    | 704512        |
| value_loss         | 0.0074510677  |
--------------------------------------
Eval num_timesteps=710000, episode_reward=16.52 +/- 8.04
Episode length: 150.00 +/- 0.00
Eval num_timesteps=720000, episode_reward=18.58 +/- 10.36
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.005478139   |
| clipfrac           | 0.060656738   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 11.4          |
| explained_variance | 0.972         |
| fps                | 1998          |
| n_updates          | 44            |
| policy_entropy     | 1.6129653     |
| policy_loss        | -0.0068429345 |
| serial_timesteps   | 90112         |
| time_elapsed       | 354           |
| total_timesteps    | 720896        |
| value_loss         | 0.0069031687  |
--------------------------------------
Eval num_timesteps=730000, episode_reward=13.75 +/- 8.58
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0050045056 |
| clipfrac           | 0.05204468   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 10.8         |
| explained_variance | 0.964        |
| fps                | 2325         |
| n_updates          | 45           |
| policy_entropy     | 1.5784578    |
| policy_loss        | -0.005658903 |
| serial_timesteps   | 92160        |
| time_elapsed       | 362          |
| total_timesteps    | 737280       |
| value_loss         | 0.0070925206 |
-------------------------------------
Eval num_timesteps=740000, episode_reward=14.87 +/- 6.53
Episode length: 150.00 +/- 0.00
Eval num_timesteps=750000, episode_reward=22.11 +/- 5.76
Episode length: 150.00 +/- 0.00
New best mean reward!
-------------------------------------
| approxkl           | 0.0053683897 |
| clipfrac           | 0.058239747  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 10.5         |
| explained_variance | 0.948        |
| fps                | 1940         |
| n_updates          | 46           |
| policy_entropy     | 1.5400467    |
| policy_loss        | -0.005662743 |
| serial_timesteps   | 94208        |
| time_elapsed       | 369          |
| total_timesteps    | 753664       |
| value_loss         | 0.0102042705 |
-------------------------------------
Eval num_timesteps=760000, episode_reward=14.32 +/- 10.99
Episode length: 150.00 +/- 0.00
Eval num_timesteps=770000, episode_reward=15.16 +/- 9.04
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.006126472   |
| clipfrac           | 0.0687439     |
| ep_len_mean        | 150           |
| ep_reward_mean     | 12.1          |
| explained_variance | 0.974         |
| fps                | 1968          |
| n_updates          | 47            |
| policy_entropy     | 1.4953783     |
| policy_loss        | -0.0072585335 |
| serial_timesteps   | 96256         |
| time_elapsed       | 377           |
| total_timesteps    | 770048        |
| value_loss         | 0.0055436837  |
--------------------------------------
Eval num_timesteps=780000, episode_reward=11.94 +/- 6.54
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0055782176 |
| clipfrac           | 0.06121826   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 12.3         |
| explained_variance | 0.976        |
| fps                | 2317         |
| n_updates          | 48           |
| policy_entropy     | 1.4506105    |
| policy_loss        | -0.006791188 |
| serial_timesteps   | 98304        |
| time_elapsed       | 386          |
| total_timesteps    | 786432       |
| value_loss         | 0.0046043163 |
-------------------------------------
Eval num_timesteps=790000, episode_reward=21.27 +/- 6.87
Episode length: 150.00 +/- 0.00
Eval num_timesteps=800000, episode_reward=21.92 +/- 6.91
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0051510227 |
| clipfrac           | 0.053753663  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 13.5         |
| explained_variance | 0.976        |
| fps                | 1881         |
| n_updates          | 49           |
| policy_entropy     | 1.4064837    |
| policy_loss        | -0.006078535 |
| serial_timesteps   | 100352       |
| time_elapsed       | 393          |
| total_timesteps    | 802816       |
| value_loss         | 0.0053842957 |
-------------------------------------
Eval num_timesteps=810000, episode_reward=20.61 +/- 6.66
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.006319786  |
| clipfrac           | 0.07485962   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 13.1         |
| explained_variance | 0.974        |
| fps                | 2208         |
| n_updates          | 50           |
| policy_entropy     | 1.3706629    |
| policy_loss        | -0.006869833 |
| serial_timesteps   | 102400       |
| time_elapsed       | 402          |
| total_timesteps    | 819200       |
| value_loss         | 0.005024492  |
-------------------------------------
Eval num_timesteps=820000, episode_reward=19.23 +/- 10.31
Episode length: 150.00 +/- 0.00
Eval num_timesteps=830000, episode_reward=20.43 +/- 11.83
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0060592615 |
| clipfrac           | 0.07091675   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 12.5         |
| explained_variance | 0.972        |
| fps                | 1973         |
| n_updates          | 51           |
| policy_entropy     | 1.3416063    |
| policy_loss        | -0.008051308 |
| serial_timesteps   | 104448       |
| time_elapsed       | 409          |
| total_timesteps    | 835584       |
| value_loss         | 0.0057945037 |
-------------------------------------
Eval num_timesteps=840000, episode_reward=14.97 +/- 8.91
Episode length: 150.00 +/- 0.00
Eval num_timesteps=850000, episode_reward=18.08 +/- 7.23
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0055883816 |
| clipfrac           | 0.061340332  |
| ep_len_mean        | 150          |
| ep_reward_mean     | 13.2         |
| explained_variance | 0.978        |
| fps                | 1769         |
| n_updates          | 52           |
| policy_entropy     | 1.3045285    |
| policy_loss        | -0.007112147 |
| serial_timesteps   | 106496       |
| time_elapsed       | 417          |
| total_timesteps    | 851968       |
| value_loss         | 0.004220908  |
-------------------------------------
Eval num_timesteps=860000, episode_reward=12.76 +/- 5.96
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0060253493  |
| clipfrac           | 0.066271976   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 14.2          |
| explained_variance | 0.976         |
| fps                | 2227          |
| n_updates          | 53            |
| policy_entropy     | 1.2543069     |
| policy_loss        | -0.0063894712 |
| serial_timesteps   | 108544        |
| time_elapsed       | 427           |
| total_timesteps    | 868352        |
| value_loss         | 0.004532269   |
--------------------------------------
Eval num_timesteps=870000, episode_reward=17.93 +/- 11.47
Episode length: 150.00 +/- 0.00
Eval num_timesteps=880000, episode_reward=17.32 +/- 9.75
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.005903365  |
| clipfrac           | 0.06739502   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 10.9         |
| explained_variance | 0.979        |
| fps                | 1958         |
| n_updates          | 54           |
| policy_entropy     | 1.2068403    |
| policy_loss        | -0.007056485 |
| serial_timesteps   | 110592       |
| time_elapsed       | 434          |
| total_timesteps    | 884736       |
| value_loss         | 0.0034816475 |
-------------------------------------
Eval num_timesteps=890000, episode_reward=20.75 +/- 9.31
Episode length: 150.00 +/- 0.00
Eval num_timesteps=900000, episode_reward=14.14 +/- 9.70
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.006572758  |
| clipfrac           | 0.0706604    |
| ep_len_mean        | 150          |
| ep_reward_mean     | 13           |
| explained_variance | 0.984        |
| fps                | 2004         |
| n_updates          | 55           |
| policy_entropy     | 1.1519191    |
| policy_loss        | -0.008003306 |
| serial_timesteps   | 112640       |
| time_elapsed       | 442          |
| total_timesteps    | 901120       |
| value_loss         | 0.0026261283 |
-------------------------------------
Eval num_timesteps=910000, episode_reward=17.33 +/- 8.51
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0056709223  |
| clipfrac           | 0.062005617   |
| ep_len_mean        | 150           |
| ep_reward_mean     | 12.8          |
| explained_variance | 0.984         |
| fps                | 2331          |
| n_updates          | 56            |
| policy_entropy     | 1.1041199     |
| policy_loss        | -0.0061927745 |
| serial_timesteps   | 114688        |
| time_elapsed       | 450           |
| total_timesteps    | 917504        |
| value_loss         | 0.0027827388  |
--------------------------------------
Eval num_timesteps=920000, episode_reward=20.06 +/- 6.16
Episode length: 150.00 +/- 0.00
Eval num_timesteps=930000, episode_reward=20.24 +/- 6.29
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0058509936 |
| clipfrac           | 0.06656494   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 14.2         |
| explained_variance | 0.986        |
| fps                | 1970         |
| n_updates          | 57           |
| policy_entropy     | 1.0687808    |
| policy_loss        | -0.005834683 |
| serial_timesteps   | 116736       |
| time_elapsed       | 458          |
| total_timesteps    | 933888       |
| value_loss         | 0.0029022575 |
-------------------------------------
Eval num_timesteps=940000, episode_reward=15.43 +/- 10.37
Episode length: 150.00 +/- 0.00
Eval num_timesteps=950000, episode_reward=22.80 +/- 8.02
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0060908822  |
| clipfrac           | 0.07052612    |
| ep_len_mean        | 150           |
| ep_reward_mean     | 15.3          |
| explained_variance | 0.987         |
| fps                | 1995          |
| n_updates          | 58            |
| policy_entropy     | 1.0347688     |
| policy_loss        | -0.0065805623 |
| serial_timesteps   | 118784        |
| time_elapsed       | 466           |
| total_timesteps    | 950272        |
| value_loss         | 0.0023957219  |
--------------------------------------
Eval num_timesteps=960000, episode_reward=16.38 +/- 6.53
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0057669706  |
| clipfrac           | 0.06428833    |
| ep_len_mean        | 150           |
| ep_reward_mean     | 13.8          |
| explained_variance | 0.988         |
| fps                | 2241          |
| n_updates          | 59            |
| policy_entropy     | 0.9962584     |
| policy_loss        | -0.0057508918 |
| serial_timesteps   | 120832        |
| time_elapsed       | 474           |
| total_timesteps    | 966656        |
| value_loss         | 0.0024634867  |
--------------------------------------
Eval num_timesteps=970000, episode_reward=19.96 +/- 9.14
Episode length: 150.00 +/- 0.00
Eval num_timesteps=980000, episode_reward=18.29 +/- 7.48
Episode length: 150.00 +/- 0.00
-------------------------------------
| approxkl           | 0.0060513243 |
| clipfrac           | 0.07075806   |
| ep_len_mean        | 150          |
| ep_reward_mean     | 15.6         |
| explained_variance | 0.989        |
| fps                | 1934         |
| n_updates          | 60           |
| policy_entropy     | 0.95951396   |
| policy_loss        | -0.006318281 |
| serial_timesteps   | 122880       |
| time_elapsed       | 481          |
| total_timesteps    | 983040       |
| value_loss         | 0.0021901545 |
-------------------------------------
Eval num_timesteps=990000, episode_reward=12.62 +/- 8.61
Episode length: 150.00 +/- 0.00
--------------------------------------
| approxkl           | 0.006709572   |
| clipfrac           | 0.07736206    |
| ep_len_mean        | 150           |
| ep_reward_mean     | 12.8          |
| explained_variance | 0.985         |
| fps                | 2234          |
| n_updates          | 61            |
| policy_entropy     | 0.9170618     |
| policy_loss        | -0.0069545596 |
| serial_timesteps   | 124928        |
| time_elapsed       | 490           |
| total_timesteps    | 999424        |
| value_loss         | 0.0024782582  |
--------------------------------------
Saving to logs/ppo2/Reacher2Dof-v0_1
pybullet build time: Mar 20 2020 20:01:46
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
