WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/acktr/acktr.py:182: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/acktr/acktr.py:224: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('ent_coef', 0.0),
             ('gamma', 0.99),
             ('learning_rate', 0.06),
             ('lr_schedule', 'constant'),
             ('n_envs', 8),
             ('n_steps', 16),
             ('n_timesteps', 1000000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('vf_coef', 0.38)])
Using 8 environments
current_dir=/ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/pybullet_envs/bullet
Normalizing input and reward
Creating test environment
Normalization activated: {'norm_reward': False}
Log path: logs/acktr/Reacher2Dof-v0_1
options= 
options= 
options= 
options= 
options= 
options= 
options= 
options= 
---------------------------------
| explained_variance | -0.0746  |
| fps                | 88       |
| nupdates           | 1        |
| policy_entropy     | 2.84     |
| policy_loss        | 0.155    |
| total_timesteps    | 128      |
| value_loss         | 11       |
---------------------------------
options= 
Eval num_timesteps=10000, episode_reward=-9.39 +/- 11.43
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -15.6    |
| explained_variance | 0.613    |
| fps                | 1565     |
| nupdates           | 100      |
| policy_entropy     | 2.87     |
| policy_loss        | 0.00776  |
| total_timesteps    | 12800    |
| value_loss         | 0.124    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-27.68 +/- 25.79
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -20.9    |
| explained_variance | 0.659    |
| fps                | 1737     |
| nupdates           | 200      |
| policy_entropy     | 2.88     |
| policy_loss        | -0.0192  |
| total_timesteps    | 25600    |
| value_loss         | 0.0577   |
---------------------------------
Eval num_timesteps=30000, episode_reward=-16.02 +/- 14.36
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -30.7    |
| explained_variance | -1.26    |
| fps                | 1836     |
| nupdates           | 300      |
| policy_entropy     | 2.88     |
| policy_loss        | 0.0318   |
| total_timesteps    | 38400    |
| value_loss         | 0.848    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-4.68 +/- 7.90
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=50000, episode_reward=-0.59 +/- 11.21
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -11.6    |
| explained_variance | 0.825    |
| fps                | 1803     |
| nupdates           | 400      |
| policy_entropy     | 2.83     |
| policy_loss        | -0.101   |
| total_timesteps    | 51200    |
| value_loss         | 0.00909  |
---------------------------------
Eval num_timesteps=60000, episode_reward=4.26 +/- 7.91
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -10.2    |
| explained_variance | 0.821    |
| fps                | 1848     |
| nupdates           | 500      |
| policy_entropy     | 2.79     |
| policy_loss        | 0.11     |
| total_timesteps    | 64000    |
| value_loss         | 0.0246   |
---------------------------------
Eval num_timesteps=70000, episode_reward=-2.46 +/- 8.79
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -8.75    |
| explained_variance | 0.0918   |
| fps                | 1883     |
| nupdates           | 600      |
| policy_entropy     | 2.75     |
| policy_loss        | 0.0204   |
| total_timesteps    | 76800    |
| value_loss         | 0.276    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-1.95 +/- 9.95
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -9.04    |
| explained_variance | 0.97     |
| fps                | 1907     |
| nupdates           | 700      |
| policy_entropy     | 2.73     |
| policy_loss        | -0.0769  |
| total_timesteps    | 89600    |
| value_loss         | 0.00295  |
---------------------------------
Eval num_timesteps=90000, episode_reward=-5.99 +/- 9.76
Episode length: 150.00 +/- 0.00
Eval num_timesteps=100000, episode_reward=-4.59 +/- 8.54
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -7.36    |
| explained_variance | 0.94     |
| fps                | 1880     |
| nupdates           | 800      |
| policy_entropy     | 2.69     |
| policy_loss        | 0.0187   |
| total_timesteps    | 102400   |
| value_loss         | 0.0052   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2.77 +/- 8.49
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -6.77    |
| explained_variance | 0.502    |
| fps                | 1900     |
| nupdates           | 900      |
| policy_entropy     | 2.66     |
| policy_loss        | -0.0803  |
| total_timesteps    | 115200   |
| value_loss         | 0.173    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-1.13 +/- 11.00
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -7.42    |
| explained_variance | 0.938    |
| fps                | 1915     |
| nupdates           | 1000     |
| policy_entropy     | 2.63     |
| policy_loss        | -0.242   |
| total_timesteps    | 128000   |
| value_loss         | 0.00135  |
---------------------------------
Eval num_timesteps=130000, episode_reward=0.97 +/- 6.22
Episode length: 150.00 +/- 0.00
Eval num_timesteps=140000, episode_reward=-3.10 +/- 13.78
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.56    |
| explained_variance | 0.927    |
| fps                | 1896     |
| nupdates           | 1100     |
| policy_entropy     | 2.63     |
| policy_loss        | 0.116    |
| total_timesteps    | 140800   |
| value_loss         | 0.00268  |
---------------------------------
Eval num_timesteps=150000, episode_reward=-3.52 +/- 9.98
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.15    |
| explained_variance | -11      |
| fps                | 1910     |
| nupdates           | 1200     |
| policy_entropy     | 2.59     |
| policy_loss        | 0.00257  |
| total_timesteps    | 153600   |
| value_loss         | 0.0738   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-3.87 +/- 9.68
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -4.26    |
| explained_variance | 0.916    |
| fps                | 1922     |
| nupdates           | 1300     |
| policy_entropy     | 2.54     |
| policy_loss        | 0.138    |
| total_timesteps    | 166400   |
| value_loss         | 0.00541  |
---------------------------------
Eval num_timesteps=170000, episode_reward=1.10 +/- 11.86
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.68    |
| explained_variance | 0.95     |
| fps                | 1932     |
| nupdates           | 1400     |
| policy_entropy     | 2.52     |
| policy_loss        | -0.0464  |
| total_timesteps    | 179200   |
| value_loss         | 0.00485  |
---------------------------------
Eval num_timesteps=180000, episode_reward=-4.74 +/- 10.33
Episode length: 150.00 +/- 0.00
Eval num_timesteps=190000, episode_reward=-1.52 +/- 6.94
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.28    |
| explained_variance | -1.04    |
| fps                | 1917     |
| nupdates           | 1500     |
| policy_entropy     | 2.51     |
| policy_loss        | -0.261   |
| total_timesteps    | 192000   |
| value_loss         | 0.0294   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2.06 +/- 9.96
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.3     |
| explained_variance | 0.839    |
| fps                | 1927     |
| nupdates           | 1600     |
| policy_entropy     | 2.45     |
| policy_loss        | -0.116   |
| total_timesteps    | 204800   |
| value_loss         | 0.00567  |
---------------------------------
Eval num_timesteps=210000, episode_reward=0.81 +/- 10.27
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.21    |
| explained_variance | 0.976    |
| fps                | 1936     |
| nupdates           | 1700     |
| policy_entropy     | 2.43     |
| policy_loss        | -0.033   |
| total_timesteps    | 217600   |
| value_loss         | 0.00132  |
---------------------------------
Eval num_timesteps=220000, episode_reward=4.03 +/- 8.18
Episode length: 150.00 +/- 0.00
Eval num_timesteps=230000, episode_reward=6.01 +/- 9.22
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.02    |
| explained_variance | -9.11    |
| fps                | 1923     |
| nupdates           | 1800     |
| policy_entropy     | 2.41     |
| policy_loss        | -0.166   |
| total_timesteps    | 230400   |
| value_loss         | 0.0974   |
---------------------------------
Eval num_timesteps=240000, episode_reward=0.26 +/- 9.90
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.6     |
| explained_variance | 0.881    |
| fps                | 1931     |
| nupdates           | 1900     |
| policy_entropy     | 2.35     |
| policy_loss        | 0.0576   |
| total_timesteps    | 243200   |
| value_loss         | 0.00371  |
---------------------------------
Eval num_timesteps=250000, episode_reward=1.94 +/- 11.66
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.67    |
| explained_variance | 0.89     |
| fps                | 1939     |
| nupdates           | 2000     |
| policy_entropy     | 2.31     |
| policy_loss        | -0.0946  |
| total_timesteps    | 256000   |
| value_loss         | 0.00279  |
---------------------------------
Eval num_timesteps=260000, episode_reward=2.80 +/- 6.31
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.15     |
| explained_variance | -9.37    |
| fps                | 1945     |
| nupdates           | 2100     |
| policy_entropy     | 2.3      |
| policy_loss        | 0.0398   |
| total_timesteps    | 268800   |
| value_loss         | 0.065    |
---------------------------------
Eval num_timesteps=270000, episode_reward=1.11 +/- 9.23
Episode length: 150.00 +/- 0.00
Eval num_timesteps=280000, episode_reward=-0.89 +/- 9.78
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.46    |
| explained_variance | 0.872    |
| fps                | 1935     |
| nupdates           | 2200     |
| policy_entropy     | 2.28     |
| policy_loss        | 0.0292   |
| total_timesteps    | 281600   |
| value_loss         | 0.000846 |
---------------------------------
Eval num_timesteps=290000, episode_reward=-1.17 +/- 10.71
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.772   |
| explained_variance | 0.752    |
| fps                | 1941     |
| nupdates           | 2300     |
| policy_entropy     | 2.23     |
| policy_loss        | 0.0144   |
| total_timesteps    | 294400   |
| value_loss         | 0.00482  |
---------------------------------
Eval num_timesteps=300000, episode_reward=1.45 +/- 8.90
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.06    |
| explained_variance | -76      |
| fps                | 1947     |
| nupdates           | 2400     |
| policy_entropy     | 2.21     |
| policy_loss        | 0.0661   |
| total_timesteps    | 307200   |
| value_loss         | 0.0571   |
---------------------------------
Eval num_timesteps=310000, episode_reward=3.44 +/- 11.15
Episode length: 150.00 +/- 0.00
Eval num_timesteps=320000, episode_reward=-1.59 +/- 10.93
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.73    |
| explained_variance | 0.767    |
| fps                | 1938     |
| nupdates           | 2500     |
| policy_entropy     | 2.21     |
| policy_loss        | 0.0378   |
| total_timesteps    | 320000   |
| value_loss         | 0.00299  |
---------------------------------
Eval num_timesteps=330000, episode_reward=1.00 +/- 8.02
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.59    |
| explained_variance | 0.798    |
| fps                | 1943     |
| nupdates           | 2600     |
| policy_entropy     | 2.16     |
| policy_loss        | 0.0757   |
| total_timesteps    | 332800   |
| value_loss         | 0.0142   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-0.02 +/- 11.76
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -2.79    |
| explained_variance | -30      |
| fps                | 1949     |
| nupdates           | 2700     |
| policy_entropy     | 2.16     |
| policy_loss        | -0.165   |
| total_timesteps    | 345600   |
| value_loss         | 0.141    |
---------------------------------
Eval num_timesteps=350000, episode_reward=1.18 +/- 8.64
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.57    |
| explained_variance | 0.985    |
| fps                | 1953     |
| nupdates           | 2800     |
| policy_entropy     | 2.12     |
| policy_loss        | -0.00473 |
| total_timesteps    | 358400   |
| value_loss         | 0.000195 |
---------------------------------
Eval num_timesteps=360000, episode_reward=0.38 +/- 11.24
Episode length: 150.00 +/- 0.00
Eval num_timesteps=370000, episode_reward=-1.07 +/- 6.13
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.44    |
| explained_variance | 0.657    |
| fps                | 1945     |
| nupdates           | 2900     |
| policy_entropy     | 2.08     |
| policy_loss        | -0.173   |
| total_timesteps    | 371200   |
| value_loss         | 0.0051   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-2.32 +/- 12.60
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -3.19    |
| explained_variance | -3.38    |
| fps                | 1949     |
| nupdates           | 3000     |
| policy_entropy     | 2.08     |
| policy_loss        | -0.0496  |
| total_timesteps    | 384000   |
| value_loss         | 0.103    |
---------------------------------
Eval num_timesteps=390000, episode_reward=3.79 +/- 10.71
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.99    |
| explained_variance | 0.826    |
| fps                | 1954     |
| nupdates           | 3100     |
| policy_entropy     | 2.04     |
| policy_loss        | -0.0299  |
| total_timesteps    | 396800   |
| value_loss         | 0.00155  |
---------------------------------
Eval num_timesteps=400000, episode_reward=0.70 +/- 9.81
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.0444  |
| explained_variance | 0.956    |
| fps                | 1958     |
| nupdates           | 3200     |
| policy_entropy     | 2.02     |
| policy_loss        | -0.122   |
| total_timesteps    | 409600   |
| value_loss         | 0.0181   |
---------------------------------
Eval num_timesteps=410000, episode_reward=6.04 +/- 7.91
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=420000, episode_reward=6.51 +/- 9.66
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -1.75    |
| explained_variance | -2.85    |
| fps                | 1949     |
| nupdates           | 3300     |
| policy_entropy     | 2.01     |
| policy_loss        | -0.115   |
| total_timesteps    | 422400   |
| value_loss         | 0.0888   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-2.74 +/- 9.29
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.925   |
| explained_variance | 0.93     |
| fps                | 1953     |
| nupdates           | 3400     |
| policy_entropy     | 1.99     |
| policy_loss        | -0.0702  |
| total_timesteps    | 435200   |
| value_loss         | 0.00449  |
---------------------------------
Eval num_timesteps=440000, episode_reward=5.89 +/- 11.48
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.635    |
| explained_variance | 0.967    |
| fps                | 1957     |
| nupdates           | 3500     |
| policy_entropy     | 1.95     |
| policy_loss        | 0.0204   |
| total_timesteps    | 448000   |
| value_loss         | 0.00232  |
---------------------------------
Eval num_timesteps=450000, episode_reward=0.72 +/- 8.85
Episode length: 150.00 +/- 0.00
Eval num_timesteps=460000, episode_reward=1.79 +/- 10.78
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.254   |
| explained_variance | -1.97    |
| fps                | 1950     |
| nupdates           | 3600     |
| policy_entropy     | 1.92     |
| policy_loss        | 0.0484   |
| total_timesteps    | 460800   |
| value_loss         | 0.0847   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-3.13 +/- 6.80
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.293   |
| explained_variance | 0.971    |
| fps                | 1954     |
| nupdates           | 3700     |
| policy_entropy     | 1.88     |
| policy_loss        | 0.127    |
| total_timesteps    | 473600   |
| value_loss         | 0.00188  |
---------------------------------
Eval num_timesteps=480000, episode_reward=-4.12 +/- 12.67
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.129   |
| explained_variance | 0.532    |
| fps                | 1958     |
| nupdates           | 3800     |
| policy_entropy     | 1.89     |
| policy_loss        | 0.0325   |
| total_timesteps    | 486400   |
| value_loss         | 0.0413   |
---------------------------------
Eval num_timesteps=490000, episode_reward=4.26 +/- 7.01
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.327   |
| explained_variance | -9.38    |
| fps                | 1961     |
| nupdates           | 3900     |
| policy_entropy     | 1.89     |
| policy_loss        | -0.0556  |
| total_timesteps    | 499200   |
| value_loss         | 0.0259   |
---------------------------------
Eval num_timesteps=500000, episode_reward=4.11 +/- 5.16
Episode length: 150.00 +/- 0.00
Eval num_timesteps=510000, episode_reward=3.71 +/- 10.93
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.281    |
| explained_variance | 0.907    |
| fps                | 1955     |
| nupdates           | 4000     |
| policy_entropy     | 1.86     |
| policy_loss        | -0.0221  |
| total_timesteps    | 512000   |
| value_loss         | 0.00351  |
---------------------------------
Eval num_timesteps=520000, episode_reward=7.35 +/- 5.97
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.899   |
| explained_variance | 0.876    |
| fps                | 1958     |
| nupdates           | 4100     |
| policy_entropy     | 1.83     |
| policy_loss        | -0.118   |
| total_timesteps    | 524800   |
| value_loss         | 0.0141   |
---------------------------------
Eval num_timesteps=530000, episode_reward=1.59 +/- 8.21
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.83     |
| explained_variance | -2.8     |
| fps                | 1961     |
| nupdates           | 4200     |
| policy_entropy     | 1.81     |
| policy_loss        | -0.0501  |
| total_timesteps    | 537600   |
| value_loss         | 0.0674   |
---------------------------------
Eval num_timesteps=540000, episode_reward=1.95 +/- 11.52
Episode length: 150.00 +/- 0.00
Eval num_timesteps=550000, episode_reward=-0.41 +/- 11.78
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.68     |
| explained_variance | 0.853    |
| fps                | 1955     |
| nupdates           | 4300     |
| policy_entropy     | 1.79     |
| policy_loss        | -0.00266 |
| total_timesteps    | 550400   |
| value_loss         | 0.00674  |
---------------------------------
Eval num_timesteps=560000, episode_reward=2.19 +/- 9.62
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.22     |
| explained_variance | 0.95     |
| fps                | 1958     |
| nupdates           | 4400     |
| policy_entropy     | 1.77     |
| policy_loss        | -0.0322  |
| total_timesteps    | 563200   |
| value_loss         | 0.0125   |
---------------------------------
Eval num_timesteps=570000, episode_reward=4.78 +/- 10.22
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.964    |
| explained_variance | 0.778    |
| fps                | 1960     |
| nupdates           | 4500     |
| policy_entropy     | 1.76     |
| policy_loss        | -0.0293  |
| total_timesteps    | 576000   |
| value_loss         | 0.0659   |
---------------------------------
Eval num_timesteps=580000, episode_reward=6.77 +/- 7.60
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.07     |
| explained_variance | 0.972    |
| fps                | 1963     |
| nupdates           | 4600     |
| policy_entropy     | 1.73     |
| policy_loss        | -0.077   |
| total_timesteps    | 588800   |
| value_loss         | 0.00313  |
---------------------------------
Eval num_timesteps=590000, episode_reward=3.47 +/- 11.13
Episode length: 150.00 +/- 0.00
Eval num_timesteps=600000, episode_reward=7.44 +/- 12.73
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.127   |
| explained_variance | 0.949    |
| fps                | 1957     |
| nupdates           | 4700     |
| policy_entropy     | 1.71     |
| policy_loss        | -0.105   |
| total_timesteps    | 601600   |
| value_loss         | 0.00919  |
---------------------------------
Eval num_timesteps=610000, episode_reward=0.70 +/- 12.64
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.967    |
| explained_variance | -92      |
| fps                | 1959     |
| nupdates           | 4800     |
| policy_entropy     | 1.69     |
| policy_loss        | 0.00497  |
| total_timesteps    | 614400   |
| value_loss         | 0.184    |
---------------------------------
Eval num_timesteps=620000, episode_reward=6.48 +/- 7.71
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.11     |
| explained_variance | 0.943    |
| fps                | 1962     |
| nupdates           | 4900     |
| policy_entropy     | 1.69     |
| policy_loss        | -0.0468  |
| total_timesteps    | 627200   |
| value_loss         | 0.00221  |
---------------------------------
Eval num_timesteps=630000, episode_reward=5.95 +/- 8.04
Episode length: 150.00 +/- 0.00
Eval num_timesteps=640000, episode_reward=5.07 +/- 7.14
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.23     |
| explained_variance | 0.901    |
| fps                | 1956     |
| nupdates           | 5000     |
| policy_entropy     | 1.68     |
| policy_loss        | -0.0918  |
| total_timesteps    | 640000   |
| value_loss         | 0.00914  |
---------------------------------
Eval num_timesteps=650000, episode_reward=0.50 +/- 8.30
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.628   |
| explained_variance | -6.96    |
| fps                | 1959     |
| nupdates           | 5100     |
| policy_entropy     | 1.66     |
| policy_loss        | 0.0147   |
| total_timesteps    | 652800   |
| value_loss         | 0.134    |
---------------------------------
Eval num_timesteps=660000, episode_reward=3.32 +/- 8.11
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.24     |
| explained_variance | 0.839    |
| fps                | 1961     |
| nupdates           | 5200     |
| policy_entropy     | 1.63     |
| policy_loss        | 0.0901   |
| total_timesteps    | 665600   |
| value_loss         | 0.0053   |
---------------------------------
Eval num_timesteps=670000, episode_reward=6.05 +/- 8.07
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 4.7      |
| explained_variance | 0.804    |
| fps                | 1964     |
| nupdates           | 5300     |
| policy_entropy     | 1.62     |
| policy_loss        | -0.0239  |
| total_timesteps    | 678400   |
| value_loss         | 0.00332  |
---------------------------------
Eval num_timesteps=680000, episode_reward=4.61 +/- 5.57
Episode length: 150.00 +/- 0.00
Eval num_timesteps=690000, episode_reward=9.72 +/- 6.76
Episode length: 150.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.22     |
| explained_variance | -64.9    |
| fps                | 1959     |
| nupdates           | 5400     |
| policy_entropy     | 1.61     |
| policy_loss        | -0.0245  |
| total_timesteps    | 691200   |
| value_loss         | 0.0359   |
---------------------------------
Eval num_timesteps=700000, episode_reward=7.13 +/- 4.94
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.23     |
| explained_variance | 0.77     |
| fps                | 1961     |
| nupdates           | 5500     |
| policy_entropy     | 1.6      |
| policy_loss        | -0.039   |
| total_timesteps    | 704000   |
| value_loss         | 0.00126  |
---------------------------------
Eval num_timesteps=710000, episode_reward=5.94 +/- 11.76
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.161    |
| explained_variance | 0.977    |
| fps                | 1964     |
| nupdates           | 5600     |
| policy_entropy     | 1.59     |
| policy_loss        | -0.0997  |
| total_timesteps    | 716800   |
| value_loss         | 0.0027   |
---------------------------------
Eval num_timesteps=720000, episode_reward=5.31 +/- 10.13
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.11     |
| explained_variance | -7.18    |
| fps                | 1966     |
| nupdates           | 5700     |
| policy_entropy     | 1.56     |
| policy_loss        | 0.109    |
| total_timesteps    | 729600   |
| value_loss         | 0.0773   |
---------------------------------
Eval num_timesteps=730000, episode_reward=5.98 +/- 7.54
Episode length: 150.00 +/- 0.00
Eval num_timesteps=740000, episode_reward=-0.98 +/- 14.59
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 0.234    |
| explained_variance | 0.985    |
| fps                | 1961     |
| nupdates           | 5800     |
| policy_entropy     | 1.54     |
| policy_loss        | 0.0777   |
| total_timesteps    | 742400   |
| value_loss         | 0.00142  |
---------------------------------
Eval num_timesteps=750000, episode_reward=7.75 +/- 13.09
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.68     |
| explained_variance | 0.93     |
| fps                | 1964     |
| nupdates           | 5900     |
| policy_entropy     | 1.54     |
| policy_loss        | -0.0985  |
| total_timesteps    | 755200   |
| value_loss         | 0.00457  |
---------------------------------
Eval num_timesteps=760000, episode_reward=9.45 +/- 9.36
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.15     |
| explained_variance | -10.4    |
| fps                | 1966     |
| nupdates           | 6000     |
| policy_entropy     | 1.51     |
| policy_loss        | -0.178   |
| total_timesteps    | 768000   |
| value_loss         | 0.0473   |
---------------------------------
Eval num_timesteps=770000, episode_reward=3.54 +/- 8.61
Episode length: 150.00 +/- 0.00
Eval num_timesteps=780000, episode_reward=-3.85 +/- 10.73
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.3      |
| explained_variance | 0.967    |
| fps                | 1961     |
| nupdates           | 6100     |
| policy_entropy     | 1.5      |
| policy_loss        | 0.0178   |
| total_timesteps    | 780800   |
| value_loss         | 0.00403  |
---------------------------------
Eval num_timesteps=790000, episode_reward=9.11 +/- 8.38
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.26     |
| explained_variance | 0.906    |
| fps                | 1963     |
| nupdates           | 6200     |
| policy_entropy     | 1.49     |
| policy_loss        | -0.133   |
| total_timesteps    | 793600   |
| value_loss         | 0.00264  |
---------------------------------
Eval num_timesteps=800000, episode_reward=3.50 +/- 9.48
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3        |
| explained_variance | -1.61    |
| fps                | 1965     |
| nupdates           | 6300     |
| policy_entropy     | 1.48     |
| policy_loss        | 0.124    |
| total_timesteps    | 806400   |
| value_loss         | 0.0679   |
---------------------------------
Eval num_timesteps=810000, episode_reward=7.07 +/- 8.34
Episode length: 150.00 +/- 0.00
----------------------------------
| ep_len_mean        | 150       |
| ep_reward_mean     | 3.2       |
| explained_variance | 0.922     |
| fps                | 1967      |
| nupdates           | 6400      |
| policy_entropy     | 1.46      |
| policy_loss        | -0.000538 |
| total_timesteps    | 819200    |
| value_loss         | 0.00209   |
----------------------------------
Eval num_timesteps=820000, episode_reward=2.81 +/- 15.53
Episode length: 150.00 +/- 0.00
Eval num_timesteps=830000, episode_reward=2.13 +/- 9.36
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.08     |
| explained_variance | 0.857    |
| fps                | 1963     |
| nupdates           | 6500     |
| policy_entropy     | 1.45     |
| policy_loss        | -0.0253  |
| total_timesteps    | 832000   |
| value_loss         | 0.00967  |
---------------------------------
Eval num_timesteps=840000, episode_reward=2.40 +/- 7.58
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.55     |
| explained_variance | -4.47    |
| fps                | 1965     |
| nupdates           | 6600     |
| policy_entropy     | 1.44     |
| policy_loss        | -0.0347  |
| total_timesteps    | 844800   |
| value_loss         | 0.0466   |
---------------------------------
Eval num_timesteps=850000, episode_reward=8.61 +/- 9.94
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.33     |
| explained_variance | 0.862    |
| fps                | 1966     |
| nupdates           | 6700     |
| policy_entropy     | 1.43     |
| policy_loss        | -0.0218  |
| total_timesteps    | 857600   |
| value_loss         | 0.00257  |
---------------------------------
Eval num_timesteps=860000, episode_reward=3.58 +/- 8.19
Episode length: 150.00 +/- 0.00
Eval num_timesteps=870000, episode_reward=4.54 +/- 7.89
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 5.1      |
| explained_variance | 0.987    |
| fps                | 1962     |
| nupdates           | 6800     |
| policy_entropy     | 1.41     |
| policy_loss        | 0.0449   |
| total_timesteps    | 870400   |
| value_loss         | 0.00643  |
---------------------------------
Eval num_timesteps=880000, episode_reward=8.31 +/- 6.90
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | -0.0853  |
| explained_variance | -1.52    |
| fps                | 1964     |
| nupdates           | 6900     |
| policy_entropy     | 1.38     |
| policy_loss        | -0.0433  |
| total_timesteps    | 883200   |
| value_loss         | 0.0639   |
---------------------------------
Eval num_timesteps=890000, episode_reward=4.02 +/- 13.60
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.94     |
| explained_variance | 0.924    |
| fps                | 1966     |
| nupdates           | 7000     |
| policy_entropy     | 1.36     |
| policy_loss        | -0.116   |
| total_timesteps    | 896000   |
| value_loss         | 0.00298  |
---------------------------------
Eval num_timesteps=900000, episode_reward=2.00 +/- 10.50
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 2.7      |
| explained_variance | 0.893    |
| fps                | 1968     |
| nupdates           | 7100     |
| policy_entropy     | 1.35     |
| policy_loss        | 0.119    |
| total_timesteps    | 908800   |
| value_loss         | 0.00841  |
---------------------------------
Eval num_timesteps=910000, episode_reward=4.72 +/- 8.73
Episode length: 150.00 +/- 0.00
Eval num_timesteps=920000, episode_reward=5.68 +/- 10.26
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 1.88     |
| explained_variance | -3.79    |
| fps                | 1964     |
| nupdates           | 7200     |
| policy_entropy     | 1.34     |
| policy_loss        | 0.136    |
| total_timesteps    | 921600   |
| value_loss         | 0.084    |
---------------------------------
Eval num_timesteps=930000, episode_reward=7.89 +/- 8.35
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.16     |
| explained_variance | 0.805    |
| fps                | 1966     |
| nupdates           | 7300     |
| policy_entropy     | 1.34     |
| policy_loss        | -0.00787 |
| total_timesteps    | 934400   |
| value_loss         | 0.00112  |
---------------------------------
Eval num_timesteps=940000, episode_reward=4.64 +/- 6.15
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 6.41     |
| explained_variance | 0.839    |
| fps                | 1967     |
| nupdates           | 7400     |
| policy_entropy     | 1.33     |
| policy_loss        | 0.0048   |
| total_timesteps    | 947200   |
| value_loss         | 0.0188   |
---------------------------------
Eval num_timesteps=950000, episode_reward=11.66 +/- 11.01
Episode length: 150.00 +/- 0.00
New best mean reward!
Eval num_timesteps=960000, episode_reward=2.37 +/- 5.67
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.68     |
| explained_variance | -3.6     |
| fps                | 1964     |
| nupdates           | 7500     |
| policy_entropy     | 1.32     |
| policy_loss        | -0.00391 |
| total_timesteps    | 960000   |
| value_loss         | 0.0567   |
---------------------------------
Eval num_timesteps=970000, episode_reward=5.54 +/- 11.26
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 4.46     |
| explained_variance | 0.854    |
| fps                | 1965     |
| nupdates           | 7600     |
| policy_entropy     | 1.32     |
| policy_loss        | -0.228   |
| total_timesteps    | 972800   |
| value_loss         | 0.00938  |
---------------------------------
Eval num_timesteps=980000, episode_reward=3.85 +/- 10.71
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 5.92     |
| explained_variance | 0.828    |
| fps                | 1967     |
| nupdates           | 7700     |
| policy_entropy     | 1.3      |
| policy_loss        | 0.0712   |
| total_timesteps    | 985600   |
| value_loss         | 0.0103   |
---------------------------------
Eval num_timesteps=990000, episode_reward=0.23 +/- 10.12
Episode length: 150.00 +/- 0.00
---------------------------------
| ep_len_mean        | 150      |
| ep_reward_mean     | 3.74     |
| explained_variance | -82.5    |
| fps                | 1969     |
| nupdates           | 7800     |
| policy_entropy     | 1.3      |
| policy_loss        | -0.119   |
| total_timesteps    | 998400   |
| value_loss         | 0.164    |
---------------------------------
Saving to logs/acktr/Reacher2Dof-v0_1
pybullet build time: Mar 20 2020 20:01:46
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
