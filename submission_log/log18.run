WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ddpg/ddpg.py:332: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ddpg/policies.py:135: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ddpg/policies.py:137: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ddpg/ddpg.py:412: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/stable_baselines/ddpg/ddpg.py:452: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('memory_limit', 50000),
             ('n_timesteps', 1000000.0),
             ('noise_std', 0.1),
             ('noise_type', 'ornstein-uhlenbeck'),
             ('policy', 'MlpPolicy')])
Using 1 environments
current_dir=/ichec/home/users/pierre/.conda/envs/base_G5/lib/python3.6/site-packages/pybullet_envs/bullet
Creating test environment
Applying ornstein-uhlenbeck noise with std 0.1
Log path: logs/ddpg/Reacher2Dof-v0_1
options= 
options= 
Eval num_timesteps=10000, episode_reward=-2.57 +/- 9.61
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -0.375   |
| reference_Q_std         | 5.51     |
| reference_action_mean   | 0.249    |
| reference_action_std    | 0.919    |
| reference_actor_Q_mean  | 0.0917   |
| reference_actor_Q_std   | 5.42     |
| rollout/Q_mean          | 1.03     |
| rollout/actions_mean    | -0.0697  |
| rollout/actions_std     | 0.869    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 66       |
| rollout/return          | -65.2    |
| rollout/return_history  | -65.2    |
| total/duration          | 26.6     |
| total/episodes          | 66       |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 376      |
| train/loss_actor        | -1.03    |
| train/loss_critic       | 3.12     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=20000, episode_reward=-5.41 +/- 10.00
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.286   |
| reference_Q_std         | 5.79     |
| reference_action_mean   | 0.218    |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 0.385    |
| reference_actor_Q_std   | 5.75     |
| rollout/Q_mean          | 1.8      |
| rollout/actions_mean    | 0.0232   |
| rollout/actions_std     | 0.786    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 133      |
| rollout/return          | -36.9    |
| rollout/return_history  | -17.7    |
| total/duration          | 53.6     |
| total/episodes          | 133      |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 373      |
| train/loss_actor        | -2.27    |
| train/loss_critic       | 1.5      |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=30000, episode_reward=-6.94 +/- 14.33
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.238    |
| reference_Q_std         | 6.58     |
| reference_action_mean   | 0.268    |
| reference_action_std    | 0.92     |
| reference_actor_Q_mean  | 0.906    |
| reference_actor_Q_std   | 6.37     |
| rollout/Q_mean          | 1.53     |
| rollout/actions_mean    | 0.0114   |
| rollout/actions_std     | 0.753    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 200      |
| rollout/return          | -28.7    |
| rollout/return_history  | -12.1    |
| total/duration          | 80.7     |
| total/episodes          | 200      |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 372      |
| train/loss_actor        | -2.37    |
| train/loss_critic       | 1.15     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=40000, episode_reward=-2.87 +/- 11.94
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.92     |
| reference_Q_std         | 6.26     |
| reference_action_mean   | 0.354    |
| reference_action_std    | 0.9      |
| reference_actor_Q_mean  | 2.73     |
| reference_actor_Q_std   | 6.29     |
| rollout/Q_mean          | 1.66     |
| rollout/actions_mean    | 0.0398   |
| rollout/actions_std     | 0.739    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 266      |
| rollout/return          | -23.2    |
| rollout/return_history  | -8.35    |
| total/duration          | 108      |
| total/episodes          | 266      |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 370      |
| train/loss_actor        | -3.03    |
| train/loss_critic       | 0.811    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=50000, episode_reward=0.13 +/- 11.06
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 2.84     |
| reference_Q_std         | 6.14     |
| reference_action_mean   | 0.24     |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 3.58     |
| reference_actor_Q_std   | 6.1      |
| rollout/Q_mean          | 1.94     |
| rollout/actions_mean    | 0.065    |
| rollout/actions_std     | 0.724    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 333      |
| rollout/return          | -19.3    |
| rollout/return_history  | -3.5     |
| total/duration          | 135      |
| total/episodes          | 333      |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 371      |
| train/loss_actor        | -3.5     |
| train/loss_critic       | 0.817    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=60000, episode_reward=4.04 +/- 7.70
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 2.94     |
| reference_Q_std         | 5.9      |
| reference_action_mean   | 0.278    |
| reference_action_std    | 0.898    |
| reference_actor_Q_mean  | 3.37     |
| reference_actor_Q_std   | 5.77     |
| rollout/Q_mean          | 2.2      |
| rollout/actions_mean    | 0.0535   |
| rollout/actions_std     | 0.713    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 400      |
| rollout/return          | -16.5    |
| rollout/return_history  | -3.27    |
| total/duration          | 162      |
| total/episodes          | 400      |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 369      |
| train/loss_actor        | -3.47    |
| train/loss_critic       | 0.53     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=70000, episode_reward=-1.88 +/- 9.99
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.15     |
| reference_Q_std         | 7.69     |
| reference_action_mean   | 0.273    |
| reference_action_std    | 0.919    |
| reference_actor_Q_mean  | 1.66     |
| reference_actor_Q_std   | 7.47     |
| rollout/Q_mean          | 2.43     |
| rollout/actions_mean    | 0.0642   |
| rollout/actions_std     | 0.706    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 466      |
| rollout/return          | -14.3    |
| rollout/return_history  | -0.727   |
| total/duration          | 190      |
| total/episodes          | 466      |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 368      |
| train/loss_actor        | -3.4     |
| train/loss_critic       | 0.289    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=80000, episode_reward=0.28 +/- 7.54
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.438   |
| reference_Q_std         | 8.79     |
| reference_action_mean   | 0.264    |
| reference_action_std    | 0.921    |
| reference_actor_Q_mean  | -0.233   |
| reference_actor_Q_std   | 8.6      |
| rollout/Q_mean          | 2.8      |
| rollout/actions_mean    | 0.11     |
| rollout/actions_std     | 0.697    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 533      |
| rollout/return          | -12.5    |
| rollout/return_history  | 0.832    |
| total/duration          | 217      |
| total/episodes          | 533      |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 368      |
| train/loss_actor        | -3.77    |
| train/loss_critic       | 0.279    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=90000, episode_reward=-4.87 +/- 9.86
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -3.37    |
| reference_Q_std         | 11       |
| reference_action_mean   | 0.409    |
| reference_action_std    | 0.865    |
| reference_actor_Q_mean  | -2.82    |
| reference_actor_Q_std   | 10.7     |
| rollout/Q_mean          | 3.12     |
| rollout/actions_mean    | 0.142    |
| rollout/actions_std     | 0.688    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 600      |
| rollout/return          | -11.5    |
| rollout/return_history  | -2.24    |
| total/duration          | 245      |
| total/episodes          | 600      |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 367      |
| train/loss_actor        | -4.82    |
| train/loss_critic       | 0.279    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=100000, episode_reward=-3.44 +/- 8.86
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.96    |
| reference_Q_std         | 10.9     |
| reference_action_mean   | 0.35     |
| reference_action_std    | 0.892    |
| reference_actor_Q_mean  | -1.4     |
| reference_actor_Q_std   | 10.6     |
| rollout/Q_mean          | 3.27     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.677    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 666      |
| rollout/return          | -10.3    |
| rollout/return_history  | -1.44    |
| total/duration          | 269      |
| total/episodes          | 666      |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 372      |
| train/loss_actor        | -4.98    |
| train/loss_critic       | 0.301    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=110000, episode_reward=1.65 +/- 10.55
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.1      |
| reference_Q_std         | 8.9      |
| reference_action_mean   | 0.293    |
| reference_action_std    | 0.919    |
| reference_actor_Q_mean  | 1.3      |
| reference_actor_Q_std   | 8.99     |
| rollout/Q_mean          | 3.44     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.663    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 733      |
| rollout/return          | -9.38    |
| rollout/return_history  | -0.294   |
| total/duration          | 292      |
| total/episodes          | 733      |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 377      |
| train/loss_actor        | -4.98    |
| train/loss_critic       | 0.231    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=120000, episode_reward=-2.58 +/- 10.78
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.139    |
| reference_Q_std         | 8.56     |
| reference_action_mean   | 0.217    |
| reference_action_std    | 0.928    |
| reference_actor_Q_mean  | 0.285    |
| reference_actor_Q_std   | 8.69     |
| rollout/Q_mean          | 3.56     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.652    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 800      |
| rollout/return          | -8.63    |
| rollout/return_history  | -0.751   |
| total/duration          | 315      |
| total/episodes          | 800      |
| total/epochs            | 1        |
| total/steps             | 119998   |
| total/steps_per_second  | 380      |
| train/loss_actor        | -4.82    |
| train/loss_critic       | 0.283    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=130000, episode_reward=-0.16 +/- 11.89
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.16    |
| reference_Q_std         | 8.18     |
| reference_action_mean   | 0.109    |
| reference_action_std    | 0.932    |
| reference_actor_Q_mean  | -0.779   |
| reference_actor_Q_std   | 8.27     |
| rollout/Q_mean          | 3.58     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.641    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 866      |
| rollout/return          | -7.74    |
| rollout/return_history  | 1.95     |
| total/duration          | 339      |
| total/episodes          | 866      |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 384      |
| train/loss_actor        | -4.59    |
| train/loss_critic       | 0.275    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=140000, episode_reward=1.07 +/- 13.60
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -3.54    |
| reference_Q_std         | 10.7     |
| reference_action_mean   | 0.272    |
| reference_action_std    | 0.925    |
| reference_actor_Q_mean  | -3.33    |
| reference_actor_Q_std   | 10.8     |
| rollout/Q_mean          | 3.65     |
| rollout/actions_mean    | 0.14     |
| rollout/actions_std     | 0.627    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 933      |
| rollout/return          | -6.82    |
| rollout/return_history  | 3.68     |
| total/duration          | 362      |
| total/episodes          | 933      |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 387      |
| train/loss_actor        | -4.28    |
| train/loss_critic       | 0.174    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=150000, episode_reward=5.28 +/- 12.50
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -2.75    |
| reference_Q_std         | 9.85     |
| reference_action_mean   | 0.282    |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | -2.53    |
| reference_actor_Q_std   | 9.86     |
| rollout/Q_mean          | 3.72     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.614    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1e+03    |
| rollout/return          | -6.06    |
| rollout/return_history  | 3.4      |
| total/duration          | 386      |
| total/episodes          | 1e+03    |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 389      |
| train/loss_actor        | -4.43    |
| train/loss_critic       | 0.318    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=160000, episode_reward=-0.78 +/- 8.36
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -4.64    |
| reference_Q_std         | 10.5     |
| reference_action_mean   | 0.287    |
| reference_action_std    | 0.905    |
| reference_actor_Q_mean  | -4.23    |
| reference_actor_Q_std   | 10.4     |
| rollout/Q_mean          | 3.73     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.604    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.07e+03 |
| rollout/return          | -5.43    |
| rollout/return_history  | 4.47     |
| total/duration          | 409      |
| total/episodes          | 1.07e+03 |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 391      |
| train/loss_actor        | -4.48    |
| train/loss_critic       | 0.224    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=170000, episode_reward=-0.58 +/- 9.99
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -5.69    |
| reference_Q_std         | 11.4     |
| reference_action_mean   | 0.363    |
| reference_action_std    | 0.877    |
| reference_actor_Q_mean  | -5.03    |
| reference_actor_Q_std   | 11.2     |
| rollout/Q_mean          | 3.71     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.595    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.13e+03 |
| rollout/return          | -4.74    |
| rollout/return_history  | 4.88     |
| total/duration          | 433      |
| total/episodes          | 1.13e+03 |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 393      |
| train/loss_actor        | -4.39    |
| train/loss_critic       | 0.279    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=180000, episode_reward=3.38 +/- 5.61
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -4.09    |
| reference_Q_std         | 10.8     |
| reference_action_mean   | 0.4      |
| reference_action_std    | 0.863    |
| reference_actor_Q_mean  | -3.42    |
| reference_actor_Q_std   | 10.7     |
| rollout/Q_mean          | 3.72     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.589    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.2e+03  |
| rollout/return          | -4.35    |
| rollout/return_history  | 3.42     |
| total/duration          | 456      |
| total/episodes          | 1.2e+03  |
| total/epochs            | 1        |
| total/steps             | 179998   |
| total/steps_per_second  | 395      |
| train/loss_actor        | -4.62    |
| train/loss_critic       | 0.17     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=190000, episode_reward=3.72 +/- 9.40
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -2.42    |
| reference_Q_std         | 10.5     |
| reference_action_mean   | 0.407    |
| reference_action_std    | 0.856    |
| reference_actor_Q_mean  | -1.73    |
| reference_actor_Q_std   | 10.3     |
| rollout/Q_mean          | 3.74     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.579    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.27e+03 |
| rollout/return          | -3.84    |
| rollout/return_history  | 4.43     |
| total/duration          | 479      |
| total/episodes          | 1.27e+03 |
| total/epochs            | 1        |
| total/steps             | 189998   |
| total/steps_per_second  | 396      |
| train/loss_actor        | -4.7     |
| train/loss_critic       | 0.2      |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=200000, episode_reward=6.28 +/- 7.60
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -1.5     |
| reference_Q_std         | 9.6      |
| reference_action_mean   | 0.377    |
| reference_action_std    | 0.866    |
| reference_actor_Q_mean  | -0.726   |
| reference_actor_Q_std   | 9.32     |
| rollout/Q_mean          | 3.76     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.57     |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.33e+03 |
| rollout/return          | -3.24    |
| rollout/return_history  | 7.13     |
| total/duration          | 503      |
| total/episodes          | 1.33e+03 |
| total/epochs            | 1        |
| total/steps             | 199998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -4.83    |
| train/loss_critic       | 0.172    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=210000, episode_reward=7.00 +/- 8.03
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 1.57     |
| reference_Q_std         | 7.86     |
| reference_action_mean   | 0.148    |
| reference_action_std    | 0.939    |
| reference_actor_Q_mean  | 2.53     |
| reference_actor_Q_std   | 7.45     |
| rollout/Q_mean          | 3.83     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.563    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.4e+03  |
| rollout/return          | -2.78    |
| rollout/return_history  | 6.8      |
| total/duration          | 526      |
| total/episodes          | 1.4e+03  |
| total/epochs            | 1        |
| total/steps             | 209998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | -4.84    |
| train/loss_critic       | 0.208    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=220000, episode_reward=8.01 +/- 7.71
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 1.58     |
| reference_Q_std         | 7.33     |
| reference_action_mean   | 0.0695   |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | 2.51     |
| reference_actor_Q_std   | 6.96     |
| rollout/Q_mean          | 3.9      |
| rollout/actions_mean    | 0.115    |
| rollout/actions_std     | 0.555    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.47e+03 |
| rollout/return          | -2.38    |
| rollout/return_history  | 6.36     |
| total/duration          | 550      |
| total/episodes          | 1.47e+03 |
| total/epochs            | 1        |
| total/steps             | 219998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -5.3     |
| train/loss_critic       | 0.209    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=230000, episode_reward=5.06 +/- 9.59
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.57     |
| reference_Q_std         | 7.36     |
| reference_action_mean   | -0.0646  |
| reference_action_std    | 0.936    |
| reference_actor_Q_mean  | 3.35     |
| reference_actor_Q_std   | 7.1      |
| rollout/Q_mean          | 4.04     |
| rollout/actions_mean    | 0.107    |
| rollout/actions_std     | 0.551    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.53e+03 |
| rollout/return          | -2.1     |
| rollout/return_history  | 4.54     |
| total/duration          | 573      |
| total/episodes          | 1.53e+03 |
| total/epochs            | 1        |
| total/steps             | 229998   |
| total/steps_per_second  | 401      |
| train/loss_actor        | -5.99    |
| train/loss_critic       | 0.335    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=240000, episode_reward=-6.86 +/- 19.18
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.04     |
| reference_Q_std         | 8.68     |
| reference_action_mean   | 0.234    |
| reference_action_std    | 0.916    |
| reference_actor_Q_mean  | 1.89     |
| reference_actor_Q_std   | 8.43     |
| rollout/Q_mean          | 4.18     |
| rollout/actions_mean    | 0.103    |
| rollout/actions_std     | 0.55     |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.6e+03  |
| rollout/return          | -2.01    |
| rollout/return_history  | 0.0913   |
| total/duration          | 597      |
| total/episodes          | 1.6e+03  |
| total/epochs            | 1        |
| total/steps             | 239998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | -6.71    |
| train/loss_critic       | 0.322    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=250000, episode_reward=12.50 +/- 12.20
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 1.41     |
| reference_Q_std         | 10.8     |
| reference_action_mean   | 0.206    |
| reference_action_std    | 0.946    |
| reference_actor_Q_mean  | 2.02     |
| reference_actor_Q_std   | 10.8     |
| rollout/Q_mean          | 4.32     |
| rollout/actions_mean    | 0.0997   |
| rollout/actions_std     | 0.546    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.67e+03 |
| rollout/return          | -1.89    |
| rollout/return_history  | 0.641    |
| total/duration          | 620      |
| total/episodes          | 1.67e+03 |
| total/epochs            | 1        |
| total/steps             | 249998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -7.35    |
| train/loss_critic       | 0.353    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=260000, episode_reward=7.27 +/- 7.66
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.673    |
| reference_Q_std         | 10.8     |
| reference_action_mean   | 0.458    |
| reference_action_std    | 0.844    |
| reference_actor_Q_mean  | 1.47     |
| reference_actor_Q_std   | 10.4     |
| rollout/Q_mean          | 4.42     |
| rollout/actions_mean    | 0.0989   |
| rollout/actions_std     | 0.541    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.73e+03 |
| rollout/return          | -1.78    |
| rollout/return_history  | 1.88     |
| total/duration          | 645      |
| total/episodes          | 1.73e+03 |
| total/epochs            | 1        |
| total/steps             | 259998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -7.35    |
| train/loss_critic       | 0.488    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=270000, episode_reward=5.04 +/- 10.77
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.71     |
| reference_Q_std         | 10       |
| reference_action_mean   | 0.536    |
| reference_action_std    | 0.801    |
| reference_actor_Q_mean  | 2.3      |
| reference_actor_Q_std   | 9.84     |
| rollout/Q_mean          | 4.51     |
| rollout/actions_mean    | 0.0972   |
| rollout/actions_std     | 0.537    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.8e+03  |
| rollout/return          | -1.71    |
| rollout/return_history  | -0.26    |
| total/duration          | 668      |
| total/episodes          | 1.8e+03  |
| total/epochs            | 1        |
| total/steps             | 269998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -7.36    |
| train/loss_critic       | 0.413    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=280000, episode_reward=-2.35 +/- 8.83
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 3.09     |
| reference_Q_std         | 9.48     |
| reference_action_mean   | 0.433    |
| reference_action_std    | 0.849    |
| reference_actor_Q_mean  | 3.7      |
| reference_actor_Q_std   | 9.37     |
| rollout/Q_mean          | 4.62     |
| rollout/actions_mean    | 0.0941   |
| rollout/actions_std     | 0.534    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.87e+03 |
| rollout/return          | -1.63    |
| rollout/return_history  | 0.199    |
| total/duration          | 693      |
| total/episodes          | 1.87e+03 |
| total/epochs            | 1        |
| total/steps             | 279998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -7.32    |
| train/loss_critic       | 0.508    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=290000, episode_reward=5.53 +/- 10.39
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.16     |
| reference_Q_std         | 9.02     |
| reference_action_mean   | 0.463    |
| reference_action_std    | 0.829    |
| reference_actor_Q_mean  | 2.64     |
| reference_actor_Q_std   | 8.98     |
| rollout/Q_mean          | 4.68     |
| rollout/actions_mean    | 0.0918   |
| rollout/actions_std     | 0.531    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 1.93e+03 |
| rollout/return          | -1.52    |
| rollout/return_history  | 0.642    |
| total/duration          | 716      |
| total/episodes          | 1.93e+03 |
| total/epochs            | 1        |
| total/steps             | 289998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -7.03    |
| train/loss_critic       | 0.43     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=300000, episode_reward=5.62 +/- 12.06
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.16     |
| reference_Q_std         | 9.14     |
| reference_action_mean   | 0.484    |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 2.34     |
| reference_actor_Q_std   | 9.27     |
| rollout/Q_mean          | 4.71     |
| rollout/actions_mean    | 0.0919   |
| rollout/actions_std     | 0.527    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2e+03    |
| rollout/return          | -1.3     |
| rollout/return_history  | 5.16     |
| total/duration          | 741      |
| total/episodes          | 2e+03    |
| total/epochs            | 1        |
| total/steps             | 299998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -6.91    |
| train/loss_critic       | 0.349    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=310000, episode_reward=3.72 +/- 14.09
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.91     |
| reference_Q_std         | 9.98     |
| reference_action_mean   | 0.445    |
| reference_action_std    | 0.852    |
| reference_actor_Q_mean  | 2.17     |
| reference_actor_Q_std   | 10.2     |
| rollout/Q_mean          | 4.76     |
| rollout/actions_mean    | 0.0909   |
| rollout/actions_std     | 0.525    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.07e+03 |
| rollout/return          | -1.05    |
| rollout/return_history  | 5.3      |
| total/duration          | 765      |
| total/episodes          | 2.07e+03 |
| total/epochs            | 1        |
| total/steps             | 309998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -7       |
| train/loss_critic       | 0.457    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=320000, episode_reward=8.07 +/- 10.06
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.04     |
| reference_Q_std         | 10.7     |
| reference_action_mean   | 0.536    |
| reference_action_std    | 0.812    |
| reference_actor_Q_mean  | 1.62     |
| reference_actor_Q_std   | 10.6     |
| rollout/Q_mean          | 4.79     |
| rollout/actions_mean    | 0.0892   |
| rollout/actions_std     | 0.522    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.13e+03 |
| rollout/return          | -0.802   |
| rollout/return_history  | 7.42     |
| total/duration          | 789      |
| total/episodes          | 2.13e+03 |
| total/epochs            | 1        |
| total/steps             | 319998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -6.78    |
| train/loss_critic       | 0.316    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=330000, episode_reward=4.91 +/- 12.07
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.68     |
| reference_Q_std         | 10.2     |
| reference_action_mean   | 0.413    |
| reference_action_std    | 0.868    |
| reference_actor_Q_mean  | 3.27     |
| reference_actor_Q_std   | 10.2     |
| rollout/Q_mean          | 4.85     |
| rollout/actions_mean    | 0.0874   |
| rollout/actions_std     | 0.52     |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.2e+03  |
| rollout/return          | -0.65    |
| rollout/return_history  | 4.89     |
| total/duration          | 813      |
| total/episodes          | 2.2e+03  |
| total/epochs            | 1        |
| total/steps             | 329998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -6.9     |
| train/loss_critic       | 0.419    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=340000, episode_reward=5.14 +/- 9.08
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 4.35     |
| reference_Q_std         | 8.55     |
| reference_action_mean   | 0.409    |
| reference_action_std    | 0.853    |
| reference_actor_Q_mean  | 4.7      |
| reference_actor_Q_std   | 8.65     |
| rollout/Q_mean          | 4.91     |
| rollout/actions_mean    | 0.0866   |
| rollout/actions_std     | 0.517    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.27e+03 |
| rollout/return          | -0.497   |
| rollout/return_history  | 3.53     |
| total/duration          | 836      |
| total/episodes          | 2.27e+03 |
| total/epochs            | 1        |
| total/steps             | 339998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -7.08    |
| train/loss_critic       | 0.347    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=350000, episode_reward=6.38 +/- 10.99
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 4.9      |
| reference_Q_std         | 7.87     |
| reference_action_mean   | 0.329    |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 5        |
| reference_actor_Q_std   | 8.16     |
| rollout/Q_mean          | 4.97     |
| rollout/actions_mean    | 0.0844   |
| rollout/actions_std     | 0.515    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.33e+03 |
| rollout/return          | -0.311   |
| rollout/return_history  | 5.15     |
| total/duration          | 860      |
| total/episodes          | 2.33e+03 |
| total/epochs            | 1        |
| total/steps             | 349998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -7.28    |
| train/loss_critic       | 0.434    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=360000, episode_reward=13.41 +/- 9.18
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 3.25     |
| reference_Q_std         | 8.4      |
| reference_action_mean   | 0.349    |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | 3.31     |
| reference_actor_Q_std   | 8.79     |
| rollout/Q_mean          | 5.02     |
| rollout/actions_mean    | 0.0821   |
| rollout/actions_std     | 0.513    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.4e+03  |
| rollout/return          | -0.214   |
| rollout/return_history  | 4.99     |
| total/duration          | 884      |
| total/episodes          | 2.4e+03  |
| total/epochs            | 1        |
| total/steps             | 359998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -7.14    |
| train/loss_critic       | 0.44     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=370000, episode_reward=5.23 +/- 8.25
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.551    |
| reference_Q_std         | 9.56     |
| reference_action_mean   | 0.245    |
| reference_action_std    | 0.933    |
| reference_actor_Q_mean  | 0.745    |
| reference_actor_Q_std   | 9.92     |
| rollout/Q_mean          | 5.08     |
| rollout/actions_mean    | 0.0799   |
| rollout/actions_std     | 0.512    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.47e+03 |
| rollout/return          | -0.0157  |
| rollout/return_history  | 5.77     |
| total/duration          | 907      |
| total/episodes          | 2.47e+03 |
| total/epochs            | 1        |
| total/steps             | 369998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -7.5     |
| train/loss_critic       | 0.419    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=380000, episode_reward=6.64 +/- 7.62
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.59     |
| reference_Q_std         | 9.13     |
| reference_action_mean   | 0.238    |
| reference_action_std    | 0.932    |
| reference_actor_Q_mean  | 2.15     |
| reference_actor_Q_std   | 9.08     |
| rollout/Q_mean          | 5.13     |
| rollout/actions_mean    | 0.0788   |
| rollout/actions_std     | 0.509    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.53e+03 |
| rollout/return          | 0.202    |
| rollout/return_history  | 8.24     |
| total/duration          | 931      |
| total/episodes          | 2.53e+03 |
| total/epochs            | 1        |
| total/steps             | 379998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -7.5     |
| train/loss_critic       | 0.37     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=390000, episode_reward=-1.25 +/- 11.64
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.38     |
| reference_Q_std         | 8.93     |
| reference_action_mean   | 0.209    |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | 3.04     |
| reference_actor_Q_std   | 8.79     |
| rollout/Q_mean          | 5.19     |
| rollout/actions_mean    | 0.0777   |
| rollout/actions_std     | 0.506    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.6e+03  |
| rollout/return          | 0.438    |
| rollout/return_history  | 8.53     |
| total/duration          | 955      |
| total/episodes          | 2.6e+03  |
| total/epochs            | 1        |
| total/steps             | 389998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -7.61    |
| train/loss_critic       | 0.387    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=400000, episode_reward=8.56 +/- 9.51
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 4.7      |
| reference_Q_std         | 8.56     |
| reference_action_mean   | 0.178    |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | 5.36     |
| reference_actor_Q_std   | 8.51     |
| rollout/Q_mean          | 5.25     |
| rollout/actions_mean    | 0.0765   |
| rollout/actions_std     | 0.504    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.67e+03 |
| rollout/return          | 0.661    |
| rollout/return_history  | 9.71     |
| total/duration          | 979      |
| total/episodes          | 2.67e+03 |
| total/epochs            | 1        |
| total/steps             | 399998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -7.65    |
| train/loss_critic       | 0.495    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=410000, episode_reward=8.83 +/- 7.12
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 5.04     |
| reference_Q_std         | 8.16     |
| reference_action_mean   | 0.228    |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | 5.39     |
| reference_actor_Q_std   | 8.22     |
| rollout/Q_mean          | 5.28     |
| rollout/actions_mean    | 0.0752   |
| rollout/actions_std     | 0.502    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.73e+03 |
| rollout/return          | 0.885    |
| rollout/return_history  | 9.77     |
| total/duration          | 1e+03    |
| total/episodes          | 2.73e+03 |
| total/epochs            | 1        |
| total/steps             | 409998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -7.34    |
| train/loss_critic       | 0.269    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=420000, episode_reward=10.42 +/- 11.56
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.29     |
| reference_Q_std         | 10.6     |
| reference_action_mean   | 0.221    |
| reference_action_std    | 0.947    |
| reference_actor_Q_mean  | 2.68     |
| reference_actor_Q_std   | 10.6     |
| rollout/Q_mean          | 5.32     |
| rollout/actions_mean    | 0.0737   |
| rollout/actions_std     | 0.5      |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.8e+03  |
| rollout/return          | 1.04     |
| rollout/return_history  | 7.96     |
| total/duration          | 1.03e+03 |
| total/episodes          | 2.8e+03  |
| total/epochs            | 1        |
| total/steps             | 419998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -7.42    |
| train/loss_critic       | 0.385    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=430000, episode_reward=7.06 +/- 8.39
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -2       |
| reference_Q_std         | 13.6     |
| reference_action_mean   | 0.233    |
| reference_action_std    | 0.936    |
| reference_actor_Q_mean  | -1.74    |
| reference_actor_Q_std   | 13.5     |
| rollout/Q_mean          | 5.38     |
| rollout/actions_mean    | 0.0728   |
| rollout/actions_std     | 0.498    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.87e+03 |
| rollout/return          | 1.22     |
| rollout/return_history  | 8.29     |
| total/duration          | 1.05e+03 |
| total/episodes          | 2.87e+03 |
| total/epochs            | 1        |
| total/steps             | 429998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -7.6     |
| train/loss_critic       | 0.491    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=440000, episode_reward=6.37 +/- 10.48
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -3.69    |
| reference_Q_std         | 14.6     |
| reference_action_mean   | 0.0896   |
| reference_action_std    | 0.954    |
| reference_actor_Q_mean  | -3.41    |
| reference_actor_Q_std   | 14.5     |
| rollout/Q_mean          | 5.43     |
| rollout/actions_mean    | 0.0724   |
| rollout/actions_std     | 0.496    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.93e+03 |
| rollout/return          | 1.29     |
| rollout/return_history  | 5        |
| total/duration          | 1.07e+03 |
| total/episodes          | 2.93e+03 |
| total/epochs            | 1        |
| total/steps             | 439998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.48    |
| train/loss_critic       | 0.333    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=450000, episode_reward=9.46 +/- 11.27
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -2.6     |
| reference_Q_std         | 13.9     |
| reference_action_mean   | 0.174    |
| reference_action_std    | 0.946    |
| reference_actor_Q_mean  | -2.08    |
| reference_actor_Q_std   | 13.7     |
| rollout/Q_mean          | 5.49     |
| rollout/actions_mean    | 0.0713   |
| rollout/actions_std     | 0.495    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3e+03    |
| rollout/return          | 1.33     |
| rollout/return_history  | 3.14     |
| total/duration          | 1.1e+03  |
| total/episodes          | 3e+03    |
| total/epochs            | 1        |
| total/steps             | 449998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.92    |
| train/loss_critic       | 0.492    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=460000, episode_reward=8.72 +/- 13.99
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.69    |
| reference_Q_std         | 13       |
| reference_action_mean   | 0.087    |
| reference_action_std    | 0.962    |
| reference_actor_Q_mean  | -0.882   |
| reference_actor_Q_std   | 12.6     |
| rollout/Q_mean          | 5.61     |
| rollout/actions_mean    | 0.0706   |
| rollout/actions_std     | 0.493    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.07e+03 |
| rollout/return          | 1.35     |
| rollout/return_history  | 1.9      |
| total/duration          | 1.12e+03 |
| total/episodes          | 3.07e+03 |
| total/epochs            | 1        |
| total/steps             | 459998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.58    |
| train/loss_critic       | 0.696    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=470000, episode_reward=5.21 +/- 7.08
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -3.02    |
| reference_Q_std         | 14.7     |
| reference_action_mean   | 0.0116   |
| reference_action_std    | 0.966    |
| reference_actor_Q_mean  | -2.72    |
| reference_actor_Q_std   | 14.6     |
| rollout/Q_mean          | 5.67     |
| rollout/actions_mean    | 0.0689   |
| rollout/actions_std     | 0.492    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.13e+03 |
| rollout/return          | 1.49     |
| rollout/return_history  | 7        |
| total/duration          | 1.15e+03 |
| total/episodes          | 3.13e+03 |
| total/epochs            | 1        |
| total/steps             | 469998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.75    |
| train/loss_critic       | 0.597    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=480000, episode_reward=5.80 +/- 4.60
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -2.5     |
| reference_Q_std         | 14.3     |
| reference_action_mean   | 0.0476   |
| reference_action_std    | 0.964    |
| reference_actor_Q_mean  | -2.11    |
| reference_actor_Q_std   | 14       |
| rollout/Q_mean          | 5.73     |
| rollout/actions_mean    | 0.0673   |
| rollout/actions_std     | 0.491    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.2e+03  |
| rollout/return          | 1.66     |
| rollout/return_history  | 8.87     |
| total/duration          | 1.17e+03 |
| total/episodes          | 3.2e+03  |
| total/epochs            | 1        |
| total/steps             | 479998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -9.03    |
| train/loss_critic       | 0.451    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=490000, episode_reward=9.76 +/- 12.77
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.203    |
| reference_Q_std         | 13.3     |
| reference_action_mean   | 0.165    |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 0.816    |
| reference_actor_Q_std   | 12.9     |
| rollout/Q_mean          | 5.8      |
| rollout/actions_mean    | 0.0662   |
| rollout/actions_std     | 0.491    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.27e+03 |
| rollout/return          | 1.74     |
| rollout/return_history  | 6.91     |
| total/duration          | 1.19e+03 |
| total/episodes          | 3.27e+03 |
| total/epochs            | 1        |
| total/steps             | 489998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -9.31    |
| train/loss_critic       | 0.593    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=500000, episode_reward=7.97 +/- 10.60
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 5.64     |
| reference_Q_std         | 9.97     |
| reference_action_mean   | 0.301    |
| reference_action_std    | 0.915    |
| reference_actor_Q_mean  | 6.35     |
| reference_actor_Q_std   | 9.43     |
| rollout/Q_mean          | 5.86     |
| rollout/actions_mean    | 0.0658   |
| rollout/actions_std     | 0.489    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.33e+03 |
| rollout/return          | 1.87     |
| rollout/return_history  | 7.3      |
| total/duration          | 1.22e+03 |
| total/episodes          | 3.33e+03 |
| total/epochs            | 1        |
| total/steps             | 499998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -9.57    |
| train/loss_critic       | 0.567    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=510000, episode_reward=5.98 +/- 13.19
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 9.3      |
| reference_Q_std         | 9.23     |
| reference_action_mean   | 0.28     |
| reference_action_std    | 0.93     |
| reference_actor_Q_mean  | 9.88     |
| reference_actor_Q_std   | 8.87     |
| rollout/Q_mean          | 5.92     |
| rollout/actions_mean    | 0.065    |
| rollout/actions_std     | 0.488    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.4e+03  |
| rollout/return          | 1.96     |
| rollout/return_history  | 5.97     |
| total/duration          | 1.24e+03 |
| total/episodes          | 3.4e+03  |
| total/epochs            | 1        |
| total/steps             | 509998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.9     |
| train/loss_critic       | 0.588    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=520000, episode_reward=7.11 +/- 10.72
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 9.76     |
| reference_Q_std         | 8.21     |
| reference_action_mean   | 0.397    |
| reference_action_std    | 0.879    |
| reference_actor_Q_mean  | 10.2     |
| reference_actor_Q_std   | 8.2      |
| rollout/Q_mean          | 5.98     |
| rollout/actions_mean    | 0.0646   |
| rollout/actions_std     | 0.487    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.47e+03 |
| rollout/return          | 2.08     |
| rollout/return_history  | 8.39     |
| total/duration          | 1.27e+03 |
| total/episodes          | 3.47e+03 |
| total/epochs            | 1        |
| total/steps             | 519998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.95    |
| train/loss_critic       | 0.588    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=530000, episode_reward=4.98 +/- 6.50
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 10.4     |
| reference_Q_std         | 8.01     |
| reference_action_mean   | 0.354    |
| reference_action_std    | 0.901    |
| reference_actor_Q_mean  | 10.8     |
| reference_actor_Q_std   | 8.18     |
| rollout/Q_mean          | 6.03     |
| rollout/actions_mean    | 0.0638   |
| rollout/actions_std     | 0.487    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.53e+03 |
| rollout/return          | 2.19     |
| rollout/return_history  | 8.72     |
| total/duration          | 1.29e+03 |
| total/episodes          | 3.53e+03 |
| total/epochs            | 1        |
| total/steps             | 529998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.92    |
| train/loss_critic       | 0.463    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=540000, episode_reward=2.87 +/- 13.91
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 8.19     |
| reference_Q_std         | 11.1     |
| reference_action_mean   | 0.418    |
| reference_action_std    | 0.882    |
| reference_actor_Q_mean  | 8.81     |
| reference_actor_Q_std   | 11.2     |
| rollout/Q_mean          | 6.1      |
| rollout/actions_mean    | 0.0621   |
| rollout/actions_std     | 0.487    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.6e+03  |
| rollout/return          | 2.25     |
| rollout/return_history  | 6.14     |
| total/duration          | 1.31e+03 |
| total/episodes          | 3.6e+03  |
| total/epochs            | 1        |
| total/steps             | 539998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -9.12    |
| train/loss_critic       | 0.618    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=550000, episode_reward=5.19 +/- 11.23
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 11.6     |
| reference_Q_std         | 8.87     |
| reference_action_mean   | 0.4      |
| reference_action_std    | 0.889    |
| reference_actor_Q_mean  | 12.4     |
| reference_actor_Q_std   | 8.95     |
| rollout/Q_mean          | 6.17     |
| rollout/actions_mean    | 0.0605   |
| rollout/actions_std     | 0.488    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.67e+03 |
| rollout/return          | 2.32     |
| rollout/return_history  | 6.62     |
| total/duration          | 1.34e+03 |
| total/episodes          | 3.67e+03 |
| total/epochs            | 1        |
| total/steps             | 549998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -9.32    |
| train/loss_critic       | 0.449    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=560000, episode_reward=7.65 +/- 12.00
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 12.6     |
| reference_Q_std         | 7        |
| reference_action_mean   | 0.383    |
| reference_action_std    | 0.894    |
| reference_actor_Q_mean  | 13       |
| reference_actor_Q_std   | 7.01     |
| rollout/Q_mean          | 6.22     |
| rollout/actions_mean    | 0.0591   |
| rollout/actions_std     | 0.487    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.73e+03 |
| rollout/return          | 2.41     |
| rollout/return_history  | 6.96     |
| total/duration          | 1.36e+03 |
| total/episodes          | 3.73e+03 |
| total/epochs            | 1        |
| total/steps             | 559998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -9.15    |
| train/loss_critic       | 0.593    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=570000, episode_reward=12.01 +/- 9.16
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 9.29     |
| reference_Q_std         | 7.17     |
| reference_action_mean   | 0.306    |
| reference_action_std    | 0.921    |
| reference_actor_Q_mean  | 9.75     |
| reference_actor_Q_std   | 7.08     |
| rollout/Q_mean          | 6.26     |
| rollout/actions_mean    | 0.0584   |
| rollout/actions_std     | 0.486    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.8e+03  |
| rollout/return          | 2.52     |
| rollout/return_history  | 9.29     |
| total/duration          | 1.39e+03 |
| total/episodes          | 3.8e+03  |
| total/epochs            | 1        |
| total/steps             | 569998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.8     |
| train/loss_critic       | 0.416    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=580000, episode_reward=7.90 +/- 14.96
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 10.6     |
| reference_Q_std         | 8.91     |
| reference_action_mean   | 0.333    |
| reference_action_std    | 0.902    |
| reference_actor_Q_mean  | 10.9     |
| reference_actor_Q_std   | 8.73     |
| rollout/Q_mean          | 6.27     |
| rollout/actions_mean    | 0.0577   |
| rollout/actions_std     | 0.484    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.87e+03 |
| rollout/return          | 2.68     |
| rollout/return_history  | 11.4     |
| total/duration          | 1.41e+03 |
| total/episodes          | 3.87e+03 |
| total/epochs            | 1        |
| total/steps             | 579998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.31    |
| train/loss_critic       | 0.61     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=590000, episode_reward=7.82 +/- 7.74
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 12.3     |
| reference_Q_std         | 11.2     |
| reference_action_mean   | 0.386    |
| reference_action_std    | 0.886    |
| reference_actor_Q_mean  | 12.6     |
| reference_actor_Q_std   | 10.9     |
| rollout/Q_mean          | 6.3      |
| rollout/actions_mean    | 0.0571   |
| rollout/actions_std     | 0.483    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 3.93e+03 |
| rollout/return          | 2.76     |
| rollout/return_history  | 9.19     |
| total/duration          | 1.44e+03 |
| total/episodes          | 3.93e+03 |
| total/epochs            | 1        |
| total/steps             | 589998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.35    |
| train/loss_critic       | 0.363    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=600000, episode_reward=11.25 +/- 9.99
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 13.3     |
| reference_Q_std         | 13.5     |
| reference_action_mean   | 0.302    |
| reference_action_std    | 0.912    |
| reference_actor_Q_mean  | 13.6     |
| reference_actor_Q_std   | 13.2     |
| rollout/Q_mean          | 6.33     |
| rollout/actions_mean    | 0.0563   |
| rollout/actions_std     | 0.481    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4e+03    |
| rollout/return          | 2.89     |
| rollout/return_history  | 9.91     |
| total/duration          | 1.46e+03 |
| total/episodes          | 4e+03    |
| total/epochs            | 1        |
| total/steps             | 599998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.03    |
| train/loss_critic       | 0.503    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=610000, episode_reward=8.98 +/- 6.76
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 10.5     |
| reference_Q_std         | 12       |
| reference_action_mean   | 0.211    |
| reference_action_std    | 0.936    |
| reference_actor_Q_mean  | 10.9     |
| reference_actor_Q_std   | 11.9     |
| rollout/Q_mean          | 6.37     |
| rollout/actions_mean    | 0.0556   |
| rollout/actions_std     | 0.479    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.07e+03 |
| rollout/return          | 3.01     |
| rollout/return_history  | 10.4     |
| total/duration          | 1.48e+03 |
| total/episodes          | 4.07e+03 |
| total/epochs            | 1        |
| total/steps             | 609998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -7.96    |
| train/loss_critic       | 0.451    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=620000, episode_reward=12.75 +/- 5.91
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 7.6      |
| reference_Q_std         | 11.6     |
| reference_action_mean   | 0.373    |
| reference_action_std    | 0.876    |
| reference_actor_Q_mean  | 8.04     |
| reference_actor_Q_std   | 11.7     |
| rollout/Q_mean          | 6.38     |
| rollout/actions_mean    | 0.0546   |
| rollout/actions_std     | 0.477    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.13e+03 |
| rollout/return          | 3.13     |
| rollout/return_history  | 9.88     |
| total/duration          | 1.51e+03 |
| total/episodes          | 4.13e+03 |
| total/epochs            | 1        |
| total/steps             | 619998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -7.51    |
| train/loss_critic       | 0.364    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=630000, episode_reward=9.47 +/- 9.32
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 9.04     |
| reference_Q_std         | 10.8     |
| reference_action_mean   | 0.324    |
| reference_action_std    | 0.899    |
| reference_actor_Q_mean  | 9.56     |
| reference_actor_Q_std   | 10.7     |
| rollout/Q_mean          | 6.4      |
| rollout/actions_mean    | 0.0535   |
| rollout/actions_std     | 0.475    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.2e+03  |
| rollout/return          | 3.23     |
| rollout/return_history  | 9.62     |
| total/duration          | 1.53e+03 |
| total/episodes          | 4.2e+03  |
| total/epochs            | 1        |
| total/steps             | 629998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -7.73    |
| train/loss_critic       | 0.189    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=640000, episode_reward=9.70 +/- 10.55
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 10.7     |
| reference_Q_std         | 11.3     |
| reference_action_mean   | 0.459    |
| reference_action_std    | 0.848    |
| reference_actor_Q_mean  | 11.3     |
| reference_actor_Q_std   | 11.2     |
| rollout/Q_mean          | 6.43     |
| rollout/actions_mean    | 0.0529   |
| rollout/actions_std     | 0.474    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.27e+03 |
| rollout/return          | 3.25     |
| rollout/return_history  | 6.31     |
| total/duration          | 1.56e+03 |
| total/episodes          | 4.27e+03 |
| total/epochs            | 1        |
| total/steps             | 639998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -8.02    |
| train/loss_critic       | 0.366    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=650000, episode_reward=8.72 +/- 7.48
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 9.99     |
| reference_Q_std         | 12.9     |
| reference_action_mean   | 0.494    |
| reference_action_std    | 0.84     |
| reference_actor_Q_mean  | 10.6     |
| reference_actor_Q_std   | 12.5     |
| rollout/Q_mean          | 6.46     |
| rollout/actions_mean    | 0.0523   |
| rollout/actions_std     | 0.473    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.33e+03 |
| rollout/return          | 3.32     |
| rollout/return_history  | 6.59     |
| total/duration          | 1.58e+03 |
| total/episodes          | 4.33e+03 |
| total/epochs            | 1        |
| total/steps             | 649998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.01    |
| train/loss_critic       | 0.278    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=660000, episode_reward=4.97 +/- 7.85
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 8.61     |
| reference_Q_std         | 13.2     |
| reference_action_mean   | 0.332    |
| reference_action_std    | 0.9      |
| reference_actor_Q_mean  | 9.05     |
| reference_actor_Q_std   | 12.8     |
| rollout/Q_mean          | 6.49     |
| rollout/actions_mean    | 0.0521   |
| rollout/actions_std     | 0.472    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.4e+03  |
| rollout/return          | 3.4      |
| rollout/return_history  | 8.01     |
| total/duration          | 1.61e+03 |
| total/episodes          | 4.4e+03  |
| total/epochs            | 1        |
| total/steps             | 659998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.16    |
| train/loss_critic       | 0.503    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=670000, episode_reward=12.95 +/- 10.80
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 7.99     |
| reference_Q_std         | 14.5     |
| reference_action_mean   | 0.257    |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 7.96     |
| reference_actor_Q_std   | 14.1     |
| rollout/Q_mean          | 6.52     |
| rollout/actions_mean    | 0.052    |
| rollout/actions_std     | 0.472    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.47e+03 |
| rollout/return          | 3.43     |
| rollout/return_history  | 6.16     |
| total/duration          | 1.63e+03 |
| total/episodes          | 4.47e+03 |
| total/epochs            | 1        |
| total/steps             | 669998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.22    |
| train/loss_critic       | 0.362    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=680000, episode_reward=6.68 +/- 13.58
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 3.94     |
| reference_Q_std         | 12.4     |
| reference_action_mean   | 0.0882   |
| reference_action_std    | 0.937    |
| reference_actor_Q_mean  | 4.1      |
| reference_actor_Q_std   | 12.4     |
| rollout/Q_mean          | 6.53     |
| rollout/actions_mean    | 0.0517   |
| rollout/actions_std     | 0.471    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.53e+03 |
| rollout/return          | 3.47     |
| rollout/return_history  | 7.09     |
| total/duration          | 1.66e+03 |
| total/episodes          | 4.53e+03 |
| total/epochs            | 1        |
| total/steps             | 679998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.91    |
| train/loss_critic       | 0.425    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=690000, episode_reward=7.62 +/- 15.16
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.87     |
| reference_Q_std         | 11.1     |
| reference_action_mean   | 0.0464   |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 3.11     |
| reference_actor_Q_std   | 11       |
| rollout/Q_mean          | 6.56     |
| rollout/actions_mean    | 0.0517   |
| rollout/actions_std     | 0.471    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.6e+03  |
| rollout/return          | 3.49     |
| rollout/return_history  | 4.13     |
| total/duration          | 1.68e+03 |
| total/episodes          | 4.6e+03  |
| total/epochs            | 1        |
| total/steps             | 689998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.72    |
| train/loss_critic       | 0.363    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=700000, episode_reward=7.03 +/- 6.94
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.87     |
| reference_Q_std         | 10.1     |
| reference_action_mean   | 0.157    |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 3.6      |
| reference_actor_Q_std   | 9.83     |
| rollout/Q_mean          | 6.57     |
| rollout/actions_mean    | 0.0515   |
| rollout/actions_std     | 0.469    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.67e+03 |
| rollout/return          | 3.53     |
| rollout/return_history  | 5.71     |
| total/duration          | 1.71e+03 |
| total/episodes          | 4.67e+03 |
| total/epochs            | 1        |
| total/steps             | 699998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.64    |
| train/loss_critic       | 0.325    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=710000, episode_reward=6.99 +/- 11.36
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 3.28     |
| reference_Q_std         | 9.96     |
| reference_action_mean   | 0.268    |
| reference_action_std    | 0.923    |
| reference_actor_Q_mean  | 3.78     |
| reference_actor_Q_std   | 9.68     |
| rollout/Q_mean          | 6.59     |
| rollout/actions_mean    | 0.0512   |
| rollout/actions_std     | 0.468    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.73e+03 |
| rollout/return          | 3.6      |
| rollout/return_history  | 7.2      |
| total/duration          | 1.73e+03 |
| total/episodes          | 4.73e+03 |
| total/epochs            | 1        |
| total/steps             | 709998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.47    |
| train/loss_critic       | 0.401    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=720000, episode_reward=0.67 +/- 14.53
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 4.03     |
| reference_Q_std         | 8.73     |
| reference_action_mean   | 0.267    |
| reference_action_std    | 0.921    |
| reference_actor_Q_mean  | 4.66     |
| reference_actor_Q_std   | 8.47     |
| rollout/Q_mean          | 6.6      |
| rollout/actions_mean    | 0.0504   |
| rollout/actions_std     | 0.466    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.8e+03  |
| rollout/return          | 3.66     |
| rollout/return_history  | 7.8      |
| total/duration          | 1.76e+03 |
| total/episodes          | 4.8e+03  |
| total/epochs            | 1        |
| total/steps             | 719998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.46    |
| train/loss_critic       | 0.392    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=730000, episode_reward=4.66 +/- 10.07
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 3.16     |
| reference_Q_std         | 8.88     |
| reference_action_mean   | 0.0195   |
| reference_action_std    | 0.95     |
| reference_actor_Q_mean  | 3.58     |
| reference_actor_Q_std   | 8.69     |
| rollout/Q_mean          | 6.63     |
| rollout/actions_mean    | 0.0496   |
| rollout/actions_std     | 0.465    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.87e+03 |
| rollout/return          | 3.74     |
| rollout/return_history  | 8.54     |
| total/duration          | 1.78e+03 |
| total/episodes          | 4.87e+03 |
| total/epochs            | 1        |
| total/steps             | 729998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.84    |
| train/loss_critic       | 0.467    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=740000, episode_reward=9.59 +/- 8.73
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.69     |
| reference_Q_std         | 9.41     |
| reference_action_mean   | -0.0546  |
| reference_action_std    | 0.955    |
| reference_actor_Q_mean  | 3.19     |
| reference_actor_Q_std   | 9.3      |
| rollout/Q_mean          | 6.64     |
| rollout/actions_mean    | 0.0489   |
| rollout/actions_std     | 0.464    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 4.93e+03 |
| rollout/return          | 3.78     |
| rollout/return_history  | 8.57     |
| total/duration          | 1.8e+03  |
| total/episodes          | 4.93e+03 |
| total/epochs            | 1        |
| total/steps             | 739998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.84    |
| train/loss_critic       | 0.481    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=750000, episode_reward=17.04 +/- 7.77
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 3.86     |
| reference_Q_std         | 9.82     |
| reference_action_mean   | 0.0316   |
| reference_action_std    | 0.972    |
| reference_actor_Q_mean  | 4.45     |
| reference_actor_Q_std   | 9.64     |
| rollout/Q_mean          | 6.65     |
| rollout/actions_mean    | 0.0484   |
| rollout/actions_std     | 0.463    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5e+03    |
| rollout/return          | 3.83     |
| rollout/return_history  | 6.86     |
| total/duration          | 1.83e+03 |
| total/episodes          | 5e+03    |
| total/epochs            | 1        |
| total/steps             | 749998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.8     |
| train/loss_critic       | 0.449    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=760000, episode_reward=6.73 +/- 14.47
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 1.85     |
| reference_Q_std         | 10.9     |
| reference_action_mean   | 0.0136   |
| reference_action_std    | 0.974    |
| reference_actor_Q_mean  | 2.38     |
| reference_actor_Q_std   | 10.8     |
| rollout/Q_mean          | 6.65     |
| rollout/actions_mean    | 0.0487   |
| rollout/actions_std     | 0.462    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.07e+03 |
| rollout/return          | 3.88     |
| rollout/return_history  | 7.6      |
| total/duration          | 1.85e+03 |
| total/episodes          | 5.07e+03 |
| total/epochs            | 1        |
| total/steps             | 759998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.9     |
| train/loss_critic       | 0.37     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=770000, episode_reward=4.77 +/- 9.43
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.67     |
| reference_Q_std         | 11.7     |
| reference_action_mean   | -0.0167  |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | 3.43     |
| reference_actor_Q_std   | 11.3     |
| rollout/Q_mean          | 6.67     |
| rollout/actions_mean    | 0.0486   |
| rollout/actions_std     | 0.461    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.13e+03 |
| rollout/return          | 3.91     |
| rollout/return_history  | 6.93     |
| total/duration          | 1.88e+03 |
| total/episodes          | 5.13e+03 |
| total/epochs            | 1        |
| total/steps             | 769998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.87    |
| train/loss_critic       | 0.339    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=780000, episode_reward=-1.42 +/- 8.67
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 2.92     |
| reference_Q_std         | 12.2     |
| reference_action_mean   | -0.0724  |
| reference_action_std    | 0.968    |
| reference_actor_Q_mean  | 3.59     |
| reference_actor_Q_std   | 12       |
| rollout/Q_mean          | 6.7      |
| rollout/actions_mean    | 0.0484   |
| rollout/actions_std     | 0.46     |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.2e+03  |
| rollout/return          | 3.92     |
| rollout/return_history  | 5.26     |
| total/duration          | 1.9e+03  |
| total/episodes          | 5.2e+03  |
| total/epochs            | 1        |
| total/steps             | 779998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -7.84    |
| train/loss_critic       | 0.311    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=790000, episode_reward=7.31 +/- 12.64
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 5.06     |
| reference_Q_std         | 10.9     |
| reference_action_mean   | 0.131    |
| reference_action_std    | 0.959    |
| reference_actor_Q_mean  | 6.06     |
| reference_actor_Q_std   | 10.6     |
| rollout/Q_mean          | 6.74     |
| rollout/actions_mean    | 0.0492   |
| rollout/actions_std     | 0.461    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.27e+03 |
| rollout/return          | 3.89     |
| rollout/return_history  | 2.81     |
| total/duration          | 1.93e+03 |
| total/episodes          | 5.27e+03 |
| total/epochs            | 1        |
| total/steps             | 789998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -8.62    |
| train/loss_critic       | 0.476    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=800000, episode_reward=0.54 +/- 19.28
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 7.14     |
| reference_Q_std         | 9.31     |
| reference_action_mean   | 0.297    |
| reference_action_std    | 0.916    |
| reference_actor_Q_mean  | 8.11     |
| reference_actor_Q_std   | 8.92     |
| rollout/Q_mean          | 6.78     |
| rollout/actions_mean    | 0.0488   |
| rollout/actions_std     | 0.462    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.33e+03 |
| rollout/return          | 3.89     |
| rollout/return_history  | 3.26     |
| total/duration          | 1.95e+03 |
| total/episodes          | 5.33e+03 |
| total/epochs            | 1        |
| total/steps             | 799998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -9.07    |
| train/loss_critic       | 0.904    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=810000, episode_reward=6.56 +/- 9.45
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 6.99     |
| reference_Q_std         | 8.7      |
| reference_action_mean   | 0.325    |
| reference_action_std    | 0.915    |
| reference_actor_Q_mean  | 7.8      |
| reference_actor_Q_std   | 8.43     |
| rollout/Q_mean          | 6.81     |
| rollout/actions_mean    | 0.0481   |
| rollout/actions_std     | 0.463    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.4e+03  |
| rollout/return          | 3.86     |
| rollout/return_history  | 2.03     |
| total/duration          | 1.98e+03 |
| total/episodes          | 5.4e+03  |
| total/epochs            | 1        |
| total/steps             | 809998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -9.38    |
| train/loss_critic       | 0.708    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=820000, episode_reward=12.18 +/- 10.70
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 6.31     |
| reference_Q_std         | 9.91     |
| reference_action_mean   | 0.296    |
| reference_action_std    | 0.936    |
| reference_actor_Q_mean  | 7.32     |
| reference_actor_Q_std   | 9.64     |
| rollout/Q_mean          | 6.85     |
| rollout/actions_mean    | 0.0481   |
| rollout/actions_std     | 0.464    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 5.47e+03 |
| rollout/return          | 3.81     |
| rollout/return_history  | 0.0846   |
| total/duration          | 2e+03    |
| total/episodes          | 5.47e+03 |
| total/epochs            | 1        |
| total/steps             | 819998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -9.53    |
| train/loss_critic       | 0.693    |
| train/param_noise_di... | 0        |
--------------------------------------

